{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe86ed7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab Assignment Seven: Recurrent Network Architectures\n",
    " \n",
    "\n",
    "#### Everett Cienkus, Blake Miller, Colin Weil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adba831",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb0229",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Define and Prepare Class Variables\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed). Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). Discuss methods of tokenization in your dataset as well as any decisions to force a specific length of sequence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967d08af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>message to examine</th>\n",
       "      <th>label (depression result)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217</td>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220</td>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>288</td>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                 message to examine  \\\n",
       "0    106  just had a real good moment. i missssssssss hi...   \n",
       "1    217         is reading manga  http://plurk.com/p/mzp1e   \n",
       "2    220  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
       "3    288  @lapcat Need to send 'em to my accountant tomo...   \n",
       "4    540      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
       "\n",
       "   label (depression result)  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('tweet_sentiments/sentiment_tweets3.csv') # read in the csv file\n",
    "X = df['message to examine']\n",
    "y = df['label (depression result)']\n",
    "\n",
    "#display\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e34a2e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23195 unique tokens. Distilled to 23196 top words.\n",
      "Shape of data tensor: (10314, 280)\n",
      "Shape of label tensor: (10314,)\n",
      "23195\n",
      "Wall time: 294 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = None # use entire vocabulary!\n",
    "MAX_TWEET_LEN = 280 # maximum and minimum number of words\n",
    "\n",
    "#tokenize the text\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X)\n",
    "# save as sequences with integers replacing words\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS)) + 1 \n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_TWEET_LEN)\n",
    "\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8d148",
   "metadata": {},
   "source": [
    "The final dataset, after preparation, is set up where each word is converted to an integer and each article is a series of integers that represent the correct ordering of words.\n",
    "\n",
    "This was accomplished by using the Keras Tokenizer as well as the Keras pad_sequences function.\n",
    "We decided on a max length of 280 characters because that is the maximum length a tweet can be.\n",
    "\n",
    "There was no need to prepare the y data because it is a binary classification that is already represented by a 0 or 1 from the csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91424e9",
   "metadata": {},
   "source": [
    "### 1.2 Choose Metric for Evaluating Peformance\n",
    "Choose and explain what metric(s) you will use to evaluate your algorithm’s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015565d6",
   "metadata": {},
   "source": [
    "We are going to choose recall as our metric for evaluating performance. We are doing this because we want to minimize false negatives which would classifying someone that has depression, as not having depression. \n",
    "\n",
    "Our business case for identifying depression from Tweets would come from the point of view of Twitter. Although we could not directly refer a person to a doctor after classifying them as having depression (because this would be a breach of privacy/ data), we could still use this information to help our users. We could send them depression therapy ads or ads that would talk about improving mental health with the hope that they would click on them and get help. It would be super important to not miss anoyone that may be depressed because we want to help all of the users possible that would have depression. \n",
    "\n",
    "On the other hand, a false positive isn't that big of a deal. In this case, a user that doesn't have depression will start to receive the mental health ads. This doesn't really make a difference in their lives and they will continue scrolling through the app without even noticing the ad.\n",
    "\n",
    "Because of these reasons, recall will be our metric for evaluating performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb04a66",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 Choose Method for Dividing Data\n",
    "Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your train/test splitting method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d622e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split it into train / test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac1236",
   "metadata": {},
   "source": [
    "Since our dataset is over 10,000, it is okay to use 80/20 split according to the Larson Rule. This allows the data to be less biased, allowing the algorithm to train with a diverse dataset. The 80/20 rule works in this case bacause the large data set almost garentees that there will be diverse data because the set should contain multiple different combinations of data. Although we are right on the cusp of having enough data, this should be okay for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f26fd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116c31e",
   "metadata": {},
   "source": [
    "### 2.1 Investigate Two Different Network Architectures\n",
    "Investigate at least two different recurrent network architectures (perhaps LSTM and GRU). Alternatively, you may also choose one recurrent network and one convolutional network. Be sure to use an embedding layer (try to use a pre-trained embedding, if possible). Adjust hyper-parameters of the networks as needed to improve generalization performance (train a total of at least four models). Discuss the performance of each network and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aead4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, SimpleRNN\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "EMBED_SIZE = 50\n",
    "RNN_STATESIZE = 20\n",
    "EPOCHS = 5\n",
    "NUM_CLASSES = 1\n",
    "input_holder = Input(shape=(X_train.shape[1], ))\n",
    "shared_embed = Embedding(top_words, # input dimension (max int of OHE)\n",
    "                EMBED_SIZE, # output dimension size\n",
    "                input_length=MAX_TWEET_LEN)(input_holder) # number of words in each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9f1ed",
   "metadata": {},
   "source": [
    "#### 2.1.1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a87fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.metrics import Recall\n",
    "\n",
    "# create LSTM\n",
    "x = LSTM(RNN_STATESIZE, dropout=0.1, recurrent_dropout=0.1)(shared_embed)\n",
    "x = Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "lstm_model = Model(inputs=input_holder,outputs=x)\n",
    "lstm_model.compile(loss='binary_crossentropy', \n",
    "              optimizer= 'adam', \n",
    "              metrics=['accuracy',tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eef046f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "129/129 [==============================] - 46s 336ms/step - loss: 0.3812 - accuracy: 0.8435 - recall: 0.3217 - val_loss: 0.1721 - val_accuracy: 0.9297 - val_recall: 0.7019\n",
      "Epoch 2/5\n",
      "129/129 [==============================] - 46s 357ms/step - loss: 0.0593 - accuracy: 0.9908 - recall: 0.9623 - val_loss: 0.0336 - val_accuracy: 0.9961 - val_recall: 0.9855\n",
      "Epoch 3/5\n",
      "129/129 [==============================] - 46s 360ms/step - loss: 0.0166 - accuracy: 0.9988 - recall: 0.9951 - val_loss: 0.0204 - val_accuracy: 0.9971 - val_recall: 0.9896\n",
      "Epoch 4/5\n",
      "129/129 [==============================] - 47s 361ms/step - loss: 0.0141 - accuracy: 0.9983 - recall: 0.9929 - val_loss: 0.0179 - val_accuracy: 0.9976 - val_recall: 0.9917\n",
      "Epoch 5/5\n",
      "129/129 [==============================] - 46s 360ms/step - loss: 0.0063 - accuracy: 0.9994 - recall: 0.9973 - val_loss: 0.0168 - val_accuracy: 0.9976 - val_recall: 0.9917\n"
     ]
    }
   ],
   "source": [
    "lstm_history = lstm_model.fit(X_train, y_train, epochs=EPOCHS, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0090d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LSTM with modified parameters\n",
    "RNN_STATESIZE = 40\n",
    "\n",
    "x = LSTM(RNN_STATESIZE, dropout=0.2, recurrent_dropout=0.2)(shared_embed)\n",
    "x = Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "lstm_model2 = Model(inputs=input_holder,outputs=x)\n",
    "lstm_model2.compile(loss='binary_crossentropy', \n",
    "              optimizer= 'adam', \n",
    "              metrics=['accuracy',tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "232bf6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "129/129 [==============================] - 58s 427ms/step - loss: 0.2003 - accuracy: 0.9306 - recall_1: 0.7198 - val_loss: 0.0371 - val_accuracy: 0.9913 - val_recall_1: 0.9648\n",
      "Epoch 2/5\n",
      "129/129 [==============================] - 55s 424ms/step - loss: 0.0099 - accuracy: 0.9992 - recall_1: 0.9962 - val_loss: 0.0173 - val_accuracy: 0.9976 - val_recall_1: 0.9896\n",
      "Epoch 3/5\n",
      "129/129 [==============================] - 55s 427ms/step - loss: 0.0046 - accuracy: 0.9995 - recall_1: 0.9978 - val_loss: 0.0238 - val_accuracy: 0.9932 - val_recall_1: 0.9938\n",
      "Epoch 4/5\n",
      "129/129 [==============================] - 55s 426ms/step - loss: 0.0063 - accuracy: 0.9990 - recall_1: 0.9962 - val_loss: 0.0123 - val_accuracy: 0.9981 - val_recall_1: 0.9917\n",
      "Epoch 5/5\n",
      "129/129 [==============================] - 54s 421ms/step - loss: 0.0028 - accuracy: 0.9995 - recall_1: 0.9978 - val_loss: 0.0125 - val_accuracy: 0.9981 - val_recall_1: 0.9917\n"
     ]
    }
   ],
   "source": [
    "lstm_history2 = lstm_model2.fit(X_train, y_train, epochs=EPOCHS, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ceb153",
   "metadata": {},
   "source": [
    "#### 2.1.2 GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd347530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GRU\n",
    "RNN_STATESIZE = 20\n",
    "x = GRU(RNN_STATESIZE, dropout=0.1, recurrent_dropout=0.1)(shared_embed)\n",
    "x = Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "gru_model = Model(inputs=input_holder,outputs=x)\n",
    "gru_model.compile(loss='binary_crossentropy', \n",
    "              optimizer= 'adam', \n",
    "              metrics=['accuracy',tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97a1d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "129/129 [==============================] - 44s 321ms/step - loss: 0.2357 - accuracy: 0.9432 - recall_2: 0.7499 - val_loss: 0.0158 - val_accuracy: 0.9971 - val_recall_2: 0.9896\n",
      "Epoch 2/5\n",
      "129/129 [==============================] - 41s 315ms/step - loss: 0.0059 - accuracy: 0.9993 - recall_2: 0.9973 - val_loss: 0.0140 - val_accuracy: 0.9981 - val_recall_2: 0.9917\n",
      "Epoch 3/5\n",
      "129/129 [==============================] - 41s 319ms/step - loss: 0.0038 - accuracy: 0.9996 - recall_2: 0.9984 - val_loss: 0.0140 - val_accuracy: 0.9981 - val_recall_2: 0.9917\n",
      "Epoch 4/5\n",
      "129/129 [==============================] - 41s 317ms/step - loss: 0.0029 - accuracy: 0.9996 - recall_2: 0.9984 - val_loss: 0.0136 - val_accuracy: 0.9981 - val_recall_2: 0.9917\n",
      "Epoch 5/5\n",
      "129/129 [==============================] - 41s 320ms/step - loss: 0.0082 - accuracy: 0.9981 - recall_2: 0.9984 - val_loss: 0.0139 - val_accuracy: 0.9981 - val_recall_2: 0.9917\n"
     ]
    }
   ],
   "source": [
    "gru_history = gru_model.fit(X_train, y_train, epochs=EPOCHS, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "988bc447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GRU with modified parameters\n",
    "RNN_STATESIZE = 40\n",
    "x = GRU(RNN_STATESIZE, dropout=0.2, recurrent_dropout=0.2)(shared_embed)\n",
    "x = Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "gru_model2 = Model(inputs=input_holder,outputs=x)\n",
    "gru_model2.compile(loss='binary_crossentropy', \n",
    "              optimizer= 'adam', \n",
    "              metrics=['accuracy',tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae4c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "129/129 [==============================] - 52s 379ms/step - loss: 0.1507 - accuracy: 0.9645 - recall_3: 0.8454 - val_loss: 0.0135 - val_accuracy: 0.9981 - val_recall_3: 0.9917\n",
      "Epoch 2/5\n",
      "129/129 [==============================] - 49s 378ms/step - loss: 0.0035 - accuracy: 0.9994 - recall_3: 0.9978 - val_loss: 0.0134 - val_accuracy: 0.9981 - val_recall_3: 0.9917\n",
      "Epoch 3/5\n",
      "129/129 [==============================] - 48s 373ms/step - loss: 0.0032 - accuracy: 0.9995 - recall_3: 0.9984 - val_loss: 0.0134 - val_accuracy: 0.9981 - val_recall_3: 0.9917\n",
      "Epoch 4/5\n",
      "129/129 [==============================] - 48s 373ms/step - loss: 0.0029 - accuracy: 0.9995 - recall_3: 0.9984 - val_loss: 0.0131 - val_accuracy: 0.9981 - val_recall_3: 0.9917\n",
      "Epoch 5/5\n",
      "129/129 [==============================] - 48s 374ms/step - loss: 0.0028 - accuracy: 0.9995 - recall_3: 0.9984 - val_loss: 0.0140 - val_accuracy: 0.9981 - val_recall_3: 0.9917\n"
     ]
    }
   ],
   "source": [
    "gru_history2 = gru_model2.fit(X_train, y_train, epochs=EPOCHS, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f71ecea",
   "metadata": {},
   "source": [
    "#### 2.1.3 Discussion and Comparison of RNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ad9f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1569a4d8",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "(FREE POINTS WOOHOO +1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb100398",
   "metadata": {},
   "source": [
    "### 2.3 Visualize Results of RNN's\n",
    "Use the method of train/test splitting and evaluation criteria that you argued for at the beginning of the lab. Visualize the results of all the RNNs you trained.  Use proper statistical comparison techniques to determine which method(s) is (are) superior.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de231f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d769933b",
   "metadata": {},
   "source": [
    "## 3. Exceptional Work \n",
    "(FREE POINTS WOOHOO +1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
