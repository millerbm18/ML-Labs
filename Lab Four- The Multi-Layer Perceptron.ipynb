{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe86ed7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab Four: The Multi-Layer Perceptron\n",
    " \n",
    "\n",
    "#### Everett Cienkus, Blake Miller, Colin Weil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adba831",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Load, Split, and Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55b544",
   "metadata": {},
   "source": [
    "#### 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18a0766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 72718 entries, 0 to 74000\n",
      "Data columns (total 37 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   TractId           72718 non-null  int64  \n",
      " 1   State             72718 non-null  int64  \n",
      " 2   County            72718 non-null  int64  \n",
      " 3   TotalPop          72718 non-null  int64  \n",
      " 4   Men               72718 non-null  int64  \n",
      " 5   Women             72718 non-null  int64  \n",
      " 6   Hispanic          72718 non-null  float64\n",
      " 7   White             72718 non-null  float64\n",
      " 8   Black             72718 non-null  float64\n",
      " 9   Native            72718 non-null  float64\n",
      " 10  Asian             72718 non-null  float64\n",
      " 11  Pacific           72718 non-null  float64\n",
      " 12  VotingAgeCitizen  72718 non-null  int64  \n",
      " 13  Income            72718 non-null  float64\n",
      " 14  IncomeErr         72718 non-null  float64\n",
      " 15  IncomePerCap      72718 non-null  float64\n",
      " 16  IncomePerCapErr   72718 non-null  float64\n",
      " 17  Poverty           72718 non-null  float64\n",
      " 18  ChildPoverty      72718 non-null  float64\n",
      " 19  Professional      72718 non-null  float64\n",
      " 20  Service           72718 non-null  float64\n",
      " 21  Office            72718 non-null  float64\n",
      " 22  Construction      72718 non-null  float64\n",
      " 23  Production        72718 non-null  float64\n",
      " 24  Drive             72718 non-null  float64\n",
      " 25  Carpool           72718 non-null  float64\n",
      " 26  Transit           72718 non-null  float64\n",
      " 27  Walk              72718 non-null  float64\n",
      " 28  OtherTransp       72718 non-null  float64\n",
      " 29  WorkAtHome        72718 non-null  float64\n",
      " 30  MeanCommute       72718 non-null  float64\n",
      " 31  Employed          72718 non-null  int64  \n",
      " 32  PrivateWork       72718 non-null  float64\n",
      " 33  PublicWork        72718 non-null  float64\n",
      " 34  SelfEmployed      72718 non-null  float64\n",
      " 35  FamilyWork        72718 non-null  float64\n",
      " 36  Unemployment      72718 non-null  float64\n",
      "dtypes: float64(29), int64(8)\n",
      "memory usage: 21.1 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data into memory and save it to a pandas data frame.\n",
    "df = pd.read_csv('census_dataset/acs2017_census_tract_data.csv')\n",
    "\n",
    "# Remove any observations that having missing data.\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode any string data as integers for now. \n",
    "le = LabelEncoder()\n",
    "s = le.fit_transform(df['State'])\n",
    "df['State'] = s\n",
    "c = le.fit_transform(df['County'])\n",
    "df['County'] = c\n",
    "\n",
    "X = df.drop(columns = ['ChildPoverty'])\n",
    "y = df['ChildPoverty']\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3442dce",
   "metadata": {},
   "source": [
    "#### 1.2 Balance (NOT DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dad64985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72718 36359.0 18179.5\n",
      "36239 36479\n",
      "18885 18736\n"
     ]
    }
   ],
   "source": [
    "print(len(y),len(y)/2,len(y)/4)\n",
    "t1 = 6.5\n",
    "t2 = 16.3\n",
    "t3 = 31\n",
    "print(len(y[y<t2]), len(y[y>=t2]))\n",
    "print(len(y[y<=t1]), len(y[y>t3]))\n",
    "\n",
    "\n",
    "\n",
    "# all factors are {1, 2, 103, 206, 353, 706, 36359, 72718}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a5cf2",
   "metadata": {},
   "source": [
    "We are going to balance training but not testing. We want to test on the distribution that would actually be sampled in order to get an accurate represenation of the results for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3875015a",
   "metadata": {},
   "source": [
    "#### 1.3 Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b74a3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 3.941e+03]\n",
      " [1.000e-01 2.200e+01]\n",
      " [2.000e-01 2.100e+01]\n",
      " ...\n",
      " [9.780e+01 1.000e+00]\n",
      " [9.830e+01 1.000e+00]\n",
      " [1.000e+02 4.800e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Divide your data into training and testing data using an 80% training \n",
    "# and 20% testing split. Use the cross validation modules that are part \n",
    "# of scikit-learn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, train_size=0.8)\n",
    "\n",
    "unique_ytrain, counts_ytrain = np.unique(y_train, return_counts=True)\n",
    "print(np.asarray((unique_ytrain, counts_ytrain)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c821cc7",
   "metadata": {},
   "source": [
    "### 2. Pre-proccesing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de8a29",
   "metadata": {},
   "source": [
    "MEGNEMAR for comparing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa3adf",
   "metadata": {},
   "source": [
    "### 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee460a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e55130d",
   "metadata": {},
   "source": [
    "### 4. Adaptive momentum (AdaM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b07ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
