{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe86ed7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab Four: The Multi-Layer Perceptron\n",
    " \n",
    "\n",
    "#### Everett Cienkus, Blake Miller, Colin Weil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adba831",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Load, Split, and Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55b544",
   "metadata": {},
   "source": [
    "#### 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18a0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data into memory and save it to a pandas data frame.\n",
    "df = pd.read_csv('census_dataset/acs2017_census_tract_data.csv')\n",
    "\n",
    "# Remove any observations that having missing data.\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode any string data as integers for now. \n",
    "le = LabelEncoder()\n",
    "s = le.fit_transform(df['State'])\n",
    "df['State'] = s\n",
    "c = le.fit_transform(df['County'])\n",
    "df['County'] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c7bb65",
   "metadata": {},
   "source": [
    "We are going to keep the county variable because this has many effects on if a child is in poverty or not. For example, somewhere like in Illinois, there can be a county that is one of the most rich counties in the country, a county that contains the inner city of Chicago, and a town in the Southwest of the state that is mostly farms. These three places are all very different and the county is a very good way to differentiate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3875015a",
   "metadata": {},
   "source": [
    "#### 1.2 Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b74a3b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Divide your data into training and testing data using an 80% training \n",
    "# and 20% testing split. Use the cross validation modules that are part \n",
    "# of scikit-learn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3442dce",
   "metadata": {},
   "source": [
    "#### 1.2 Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad64985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2 16.3 31.6\n",
      "[['A' 14579]\n",
      " ['B' 14445]\n",
      " ['C' 14580]\n",
      " ['D' 14570]]\n",
      "58174\n"
     ]
    }
   ],
   "source": [
    "# Uses quantization thresholds for the \"ChildPoverty\" \n",
    "# variable that equally divide the data into four classes\n",
    "q1, q2, q3 = df['ChildPoverty'].quantile([0.25,0.5,0.75])\n",
    "print(q1,q2,q3)\n",
    "df_train['quantized_childPoverty'] = pd.cut(x=df_train['ChildPoverty'], bins=[-1,q1,q2,q3,101], labels=['A','B','C','D'])\n",
    "X_train = df_train.drop(columns = ['ChildPoverty','quantized_childPoverty']).to_numpy()\n",
    "y_train = df_train['quantized_childPoverty'].to_numpy()\n",
    "\n",
    "df_test['quantized_childPoverty'] = pd.cut(x=df_test['ChildPoverty'], bins=[-1,q1,q2,q3,101], labels=['A','B','C','D'])\n",
    "X_test = df_test.drop(columns = ['ChildPoverty','quantized_childPoverty']).to_numpy()\n",
    "y_test = df_test['quantized_childPoverty'].to_numpy()\n",
    "\n",
    "unique_ytrain, counts_ytrain = np.unique(y_train, return_counts=True)\n",
    "print(np.asarray((unique_ytrain, counts_ytrain)).T)\n",
    "print(len(y_train))\n",
    "\n",
    "\n",
    "\n",
    "# df_train['quantized_childPoverty'] = pd.qcut(df_train['ChildPoverty'], 4, labels=['A','B','C','D'])\n",
    "# X_train = df_train.drop(columns = ['ChildPoverty','quantized_childPoverty'])\n",
    "# y_train = df_train['quantized_childPoverty'] \n",
    "# unique_ytrain, counts_ytrain = np.unique(y_train, return_counts=True)\n",
    "# print(np.asarray((unique_ytrain, counts_ytrain)).T)\n",
    "# print(len(y_train))\n",
    "# OUTPUTS \n",
    "# [['A' 14725]\n",
    "#  ['B' 14401]\n",
    "#  ['C' 14521]\n",
    "#  ['D' 14527]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a5cf2",
   "metadata": {},
   "source": [
    "We balance the training and not the testing set because it is good to balance for training because then we have information for all of our classes but when we are testing, we want to test on the distributuion that would actually be the case in order to get an accurate represenation of the results for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c821cc7",
   "metadata": {},
   "source": [
    "### 2. Pre-proccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3a8165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def print_result(nn,X_train,y_train,X_test,y_test,title=\"\",color=\"red\"):\n",
    "    \n",
    "    print(\"=================\")\n",
    "    print(title,\":\")\n",
    "    yhat = nn.predict(X_train)\n",
    "    print('Resubstitution acc:',accuracy_score(y_train,yhat))\n",
    "    \n",
    "    yhat = nn.predict(X_test)\n",
    "    print('Validation acc:',accuracy_score(y_test,yhat))\n",
    "    \n",
    "    if hasattr(nn,'val_score_'):\n",
    "        plt.plot(range(len(nn.val_score_)), nn.val_score_, color=color,label=title)\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "    else:\n",
    "        plt.plot(range(len(nn.score_)), nn.score_, color=color,label=title)\n",
    "        plt.ylabel('Resub Accuracy')\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d00de5",
   "metadata": {},
   "source": [
    "#### 2.1 Two-Layer Perceptron Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8703a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_) # reshape to be W\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden)\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80eeadfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# just start with the vectorized version and minibatch\n",
    "class TLPMiniBatch(TwoLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.1, \n",
    "                 decrease_iter = 10, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2)\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2 = eta * gradW1, eta * gradW2\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                rho_W1_prev, rho_W2_prev = rho_W1, rho_W2\n",
    "                \n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "            \n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a883d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to implement the new style of objective function, \n",
    "# we just need to update the final layer calculation of the gradient\n",
    "class TLPMiniBatchCrossEntropy(TLPMiniBatch):\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = (A3-Y_enc) # <- this is only line that changed\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4a9af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLPBetterInitial(TLPMiniBatchCrossEntropy):             \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden)) \n",
    "        \n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e01bd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 3.08 s, total: 15 s\n",
      "Wall time: 3.32 s\n",
      "=================\n",
      "Glorot Initial :\n",
      "Resubstitution acc: 0.0\n",
      "Validation acc: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vals = { 'n_hidden':30, \n",
    "         'C':0.1, 'epochs':20, 'eta':0.001, # poor starting learning rate!!\n",
    "         'alpha':0.001, 'decrease_const':0.1, 'decrease_iter':15,\n",
    "         'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn_better = TLPBetterInitial(**vals)\n",
    "%time nn_better.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "print_result(nn_better,X_train,y_train,X_test,y_test,title=\"Glorot Initial\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8a6b7",
   "metadata": {},
   "source": [
    "#### 2.2 TLPNetwork with Normalizing the Continuous Numeric Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9830a5cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['quantized_childPoverty'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize\n\u001b[1;32m      3\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTractId\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCounty\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      4\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m      5\u001b[0m     (X_normalized, \n\u001b[0;32m----> 6\u001b[0m     normalize(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTractId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mState\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCounty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mChildPoverty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquantized_childPoverty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy())),\n\u001b[1;32m      7\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_normalized,y,test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m     11\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnn_better.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4806\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   4807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   4808\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4815\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4816\u001b[0m ):\n\u001b[1;32m   4817\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4818\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   4819\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4952\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   4953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4956\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4267\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4311\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4312\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4314\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6644\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6645\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['quantized_childPoverty'] not found in axis\""
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# X_normalized = df[['TractId','State','County']]\n",
    "# X_normalized = np.concatenate(\n",
    "#     (X_normalized, \n",
    "#     normalize(df.drop(columns = ['TractId','State','County','ChildPoverty','quantized_childPoverty']).to_numpy())),\n",
    "#     axis=1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_normalized,y,test_size=0.2, train_size=0.8)\n",
    "\n",
    "# %time nn_better.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "# print_result(nn_better,X_train,y_train,X_test,y_test,title=\"Glorot Initial\",color=\"blue\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6106ef76",
   "metadata": {},
   "source": [
    "#### 2.3 TLPNetwork with Normalizing + One Hot Encode the Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode the Categorical Data\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"embarked\"\n",
    "# tmp_df = pd.get_dummies(df_imputed.Embarked,prefix='Embarked')\n",
    "# df_imputed = pd.concat((df_imputed,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "# tmp_df = pd.get_dummies(df_imputed.Sex,prefix='Sex')\n",
    "# df_imputed = pd.concat((df_imputed,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "\n",
    "%time nn_better.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "\n",
    "print_result(nn_better,X_train,y_train,X_test,y_test,title=\"Glorot Initial\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a8e48",
   "metadata": {},
   "source": [
    "#### 2.4 Comparing the Performance of the Three Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921e6e5",
   "metadata": {},
   "source": [
    "Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa3adf",
   "metadata": {},
   "source": [
    "### 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf5e64",
   "metadata": {},
   "source": [
    "Feed forward/ back propogation in for-loop for adding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f1fd4",
   "metadata": {},
   "source": [
    "MEGNEMAR for comparing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e55130d",
   "metadata": {},
   "source": [
    "### 4. Adaptive momentum (AdaM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcdf01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
