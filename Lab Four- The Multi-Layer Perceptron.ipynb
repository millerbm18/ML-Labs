{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe86ed7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab Four: The Multi-Layer Perceptron\n",
    " \n",
    "\n",
    "#### Everett Cienkus, Blake Miller, Colin Weil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adba831",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Load, Split, and Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55b544",
   "metadata": {},
   "source": [
    "#### 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb408ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data into memory and save it to a pandas data frame.\n",
    "df = pd.read_csv('census_dataset/acs2017_census_tract_data.csv')\n",
    "\n",
    "# Remove any observations that having missing data.\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode any string data as integers for now. \n",
    "le = LabelEncoder()\n",
    "df['State'] = le.fit_transform(df['State'])\n",
    "df['County'] = le.fit_transform(df['County'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27bd78",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are going to keep the county variable because this has many effects on if a child is in poverty or not. For example, somewhere like in Illinois, there can be a county that is one of the most rich counties in the country, a county that contains the inner city of Chicago, and a town in the Southwest of the state that is mostly farms. These three places are all very different and the county is a very good way to differentiate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491990a0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2 Split Dataset and Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749fac38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Divide your data into training and testing data using an 80% training \n",
    "# and 20% testing split. Use the cross validation modules that are part \n",
    "# of scikit-learn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, train_size=0.8)\n",
    "\n",
    "# Uses quantization thresholds for the \"ChildPoverty\" \n",
    "# variable that equally divide the data into four classes\n",
    "q1, q2, q3 = df_train['ChildPoverty'].quantile([0.25,0.5,0.75])\n",
    "print(q1,q2,q3)\n",
    "df_train['quantized_childPoverty'] = pd.cut(x=df_train['ChildPoverty'], bins=[-1,q1,q2,q3,101], labels=[0,1,2,3])\n",
    "X_train = df_train.drop(columns = ['ChildPoverty','quantized_childPoverty','TractId']).to_numpy()\n",
    "y_train = df_train['quantized_childPoverty'].to_numpy()\n",
    "\n",
    "df_test['quantized_childPoverty'] = pd.cut(x=df_test['ChildPoverty'], bins=[-1,q1,q2,q3,101], labels=[0,1,2,3])\n",
    "X_test = df_test.drop(columns = ['ChildPoverty','quantized_childPoverty','TractId']).to_numpy()\n",
    "y_test = df_test['quantized_childPoverty'].to_numpy()\n",
    "\n",
    "unique_ytrain, counts_ytrain = np.unique(y_train, return_counts=True)\n",
    "print(np.asarray((unique_ytrain, counts_ytrain)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a7ca89",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We balance the training and not the testing set because it is good to balance for training because then we have information for all of our classes but when we are testing, we want to test on the distributuion that would actually be the case in order to get an accurate represenation of the results for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701fbe1a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Pre-proccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ed1f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_) # reshape to be W\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden)\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda9ba2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# just start with the vectorized version and minibatch\n",
    "class TLPMiniBatch(TwoLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.1, \n",
    "                 decrease_iter = 10, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2\n",
    "                                                      )\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2)\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2 = eta * gradW1, eta * gradW2\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                rho_W1_prev, rho_W2_prev = rho_W1, rho_W2\n",
    "                \n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "            \n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d789e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# to implement the new style of objective function, \n",
    "# we just need to update the final layer calculation of the gradient\n",
    "class TLPMiniBatchCrossEntropy(TLPMiniBatch):\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = (A3-Y_enc) # <- this is only line that changed\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec0024e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TLPBetterInitial(TLPMiniBatchCrossEntropy):             \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden)) \n",
    "        \n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977ac40",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def print_result(nn,X_train,y_train,X_test,y_test,title=\"\",color=\"red\"):\n",
    "    \n",
    "    print(\"=================\")\n",
    "    print(title,\":\")\n",
    "    yhat = nn.predict(X_train)\n",
    "    print('Resubstitution acc:',accuracy_score(y_train,yhat))\n",
    "    \n",
    "    yhat = nn.predict(X_test)\n",
    "    print('Validation acc:',accuracy_score(y_test,yhat))\n",
    "    \n",
    "    if hasattr(nn,'val_score_'):\n",
    "        plt.plot(range(len(nn.val_score_)), nn.val_score_, color=color,label=title)\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "    else:\n",
    "        plt.plot(range(len(nn.score_)), nn.score_, color=color,label=title)\n",
    "        plt.ylabel('Resub Accuracy')\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c28e7a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1 Two-Layer Perceptron Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bab0d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7/200"
     ]
    }
   ],
   "source": [
    "vals = { 'n_hidden':30,\n",
    "         'C':0.1, 'epochs':200, 'eta':0.001, # poor starting learning rate!!\n",
    "         'alpha':0.001, 'decrease_const':1e-5,\n",
    "         'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "tlpPlain = TLPBetterInitial(**vals)\n",
    "%time tlpPlain.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "\n",
    "print_result(tlpPlain,X_train,y_train,X_test,y_test,title=\"No Normalization/ One Hot\",color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534de2d0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.2 TLPNetwork with Normalizing the Continuous Numeric Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c55c7a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_normalized = ss.transform(X_train)\n",
    "ss.fit(X_test)\n",
    "X_test_normalized = ss.transform(X_test)\n",
    "\n",
    "tlpNormal = TLPBetterInitial(**vals)\n",
    "%time tlpNormal.fit(X_train_normalized, y_train, print_progress=1, XY_test=(X_test_normalized, y_test))\n",
    "\n",
    "print_result(tlpNormal,X_train_normalized,y_train,X_test_normalized,y_test,title=\"Normalization\",color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86306f",
   "metadata": {},
   "source": [
    "#### 2.3 TLPNetwork with Normalizing and One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add4163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# One Hot Encode the Categorical Data\n",
    "\n",
    "states_arr = np.concatenate((X_train[:,0], X_test[:,0]))\n",
    "states_hot_encode = pd.get_dummies(states_arr)\n",
    "train_states = states_hot_encode[0:len(X_train)]\n",
    "test_states = states_hot_encode[len(X_train):len(X_train)+len(X_test)]\n",
    "\n",
    "counties_arr = np.concatenate((X_train[:,1], X_test[:,1]))\n",
    "counties_hot_encode = pd.get_dummies(counties_arr)\n",
    "train_counties = counties_hot_encode[0:len(X_train)]\n",
    "test_counties = counties_hot_encode[len(X_train):len(X_train)+len(X_test)]\n",
    "\n",
    "train_one_hot = np.concatenate((train_states,train_counties), axis=1)\n",
    "X_train_normal_hot = np.concatenate(( train_one_hot, X_train_normalized[:,2:len(X_train_normalized[0])]), axis=1)      \n",
    "\n",
    "test_one_hot = np.concatenate((test_states,test_counties), axis=1)\n",
    "X_test_normal_hot = np.concatenate(( test_one_hot, X_test_normalized[:,2:len(X_test_normalized[0])]), axis=1) \n",
    "\n",
    "tlpNormHot = TLPBetterInitial(**vals)\n",
    "%time tlpNormHot.fit(X_train_normal_hot, y_train, print_progress=1, XY_test=(X_test_normal_hot, y_test))\n",
    "\n",
    "print_result(tlpNormHot,X_train_normal_hot,y_train,X_test_normal_hot,y_test,title=\"Normalization + One Hot\",color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a8e48",
   "metadata": {},
   "source": [
    "#### 2.4 Comparing the Performance of the Three Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_result(tlpPlain,X_train,y_train,X_test,y_test,title=\"No Normalization/ One Hot\",color=\"blue\")\n",
    "print_result(tlpNormal,X_train,y_train,X_test,y_test,title=\"Normalization\",color=\"green\")\n",
    "print_result(tlpNormHot,X_train_normal_hot,y_train,X_test_normal_hot,y_test,title=\"Normalization + One Hot\",color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921e6e5",
   "metadata": {},
   "source": [
    "Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa3adf",
   "metadata": {},
   "source": [
    "### 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf5e64",
   "metadata": {},
   "source": [
    "Feed forward/ back propogation in for-loop for adding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f1fd4",
   "metadata": {},
   "source": [
    "MEGNEMAR for comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338ee4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ThreeLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30, n_hidden2 = 20,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_hidden2 + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_hidden2, self.n_hidden))\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden2))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden2))\n",
    "\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_hidden2, 1))\n",
    "        b3 = np.zeros((self.n_output_, 1))\n",
    "\n",
    "        return W1, W2, W3, b1, b2, b3\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2)+ np.mean(W3[:, 1:] ** 2))\n",
    "\n",
    "    def _cost(self,A4,Y_enc,W1,W2,W3):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A4)+(1-Y_enc)*np.log(1-A4))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _feedforward(self, X, W1, W2, W3, b1, b2, b3):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs\n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        Z3 = W3 @ A3 + b3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, A4, Z1, Z2, Z3, Y_enc, W1, W2, W3):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V3 = -2*(Y_enc-A4)*A4*(1-A4)\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "\n",
    "        gradW3 = V3 @ A3.T\n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "\n",
    "        gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "        gradW3 += W3 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradW3, gradb1, gradb2, gradb3\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, A4 = self._feedforward(X, self.W1, self.W2, self.W3, self.b1, self.b2, self.b3)\n",
    "        y_pred = np.argmax(A4, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4125124",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ThreeLPMiniBatch(ThreeLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.1,\n",
    "                 decrease_iter = 10, shuffle=True,\n",
    "                 minibatches=1, **kwds):\n",
    "        # need to add to the original initializer\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.b1, self.b2, self.b3 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "        rho_W3_prev = np.zeros(self.W3.shape) # for momentum\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2,\n",
    "                                                       self.W3,\n",
    "                                                       self.b1,\n",
    "                                                       self.b2,\n",
    "                                                       self.b3\n",
    "                                                       )\n",
    "\n",
    "                cost = self._cost(A4,Y_enc[:, idx],self.W1,self.W2,self.W3)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradW3, gradb1, gradb2, gradb3 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4, Z1=Z1, Z2=Z2, Z3=Z3,\n",
    "                                                                    Y_enc=Y_enc[:, idx],\n",
    "                                                                    W1=self.W1,W2=self.W2,W3=self.W3)\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2, rho_W3 = eta * gradW1, eta * gradW2, eta * gradW3\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev)) # update with momentum\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                self.b3 -= eta * gradb3\n",
    "                rho_W1_prev, rho_W2_prev, rho_W3_prev = rho_W1, rho_W2, rho_W3\n",
    "\n",
    "            self.grad_w1_[i] = np.mean(gradW1)\n",
    "            self.grad_w2_[i] = np.mean(gradW2)\n",
    "            self.grad_w3_[i] = np.mean(gradW3)\n",
    "\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76a002",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "threeLayer = ThreeLPMiniBatch(**vals)\n",
    "%time threeLayer.fit(X_train_normal_hot, y_train, print_progress=1, XY_test=(X_test_normal_hot, y_test))\n",
    "plt.plot(abs(threeLayer.grad_w1_[10:]), label='w1')\n",
    "plt.plot(abs(threeLayer.grad_w2_[10:]), label='w2')\n",
    "plt.plot(abs(threeLayer.grad_w3_[10:]), label='w3')\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ab4de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_result(threeLayer,X_train_normal_hot,y_train,X_test_normal_hot,y_test,title=\"Normalization\",color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c62058",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FourLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30, n_hidden2 = 25, n_hidden3 = 25,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_hidden3 = n_hidden3\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_hidden2 + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_hidden2, self.n_hidden))\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_hidden3 + self.n_hidden2))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n_hidden3, self.n_hidden2))\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden3))\n",
    "        W4 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden3))\n",
    "\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_hidden2, 1))\n",
    "        b3 = np.zeros((self.n_hidden3, 1))\n",
    "        b4 = np.zeros((self.n_output_, 1))\n",
    "\n",
    "        return W1, W2, W3, W4, b1, b2, b3, b4\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3, W4):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2)+ np.mean(W3[:, 1:] ** 2)+ np.mean(W4[:, 1:] ** 2))\n",
    "\n",
    "    def _cost(self,A5,Y_enc,W1,W2,W3,W4):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A5)+(1-Y_enc)*np.log(1-A5))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _feedforward(self, X, W1, W2, W3, W4, b1, b2, b3, b4):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs\n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        Z3 = W3 @ A3 + b3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        Z4 = W4 @ A4 + b4\n",
    "        A5 = self._sigmoid(Z4)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, A4, A5,Z1, Z2, Z3, Z4, Y_enc, W1, W2, W3, W4):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V4 = -2*(Y_enc-A5)*A5*(1-A5)\n",
    "        V3 = A4*(1-A4)*(W4.T @ V4)\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "\n",
    "        gradW4 = V4 @ A4.T\n",
    "        gradW3 = V3 @ A3.T\n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "\n",
    "        gradb4 = np.sum(V4, axis=1).reshape((-1,1))\n",
    "        gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "        gradW3 += W3 * self.l2_C\n",
    "        gradW4 += W4 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradW3, gradW4, gradb1, gradb2, gradb3, gradb4\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, _,_,A5 = self._feedforward(X, self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4)\n",
    "        y_pred = np.argmax(A5, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c4323",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FourLPMiniBatch(FourLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.1,\n",
    "                 decrease_iter = 10, shuffle=True,\n",
    "                 minibatches=1, **kwds):\n",
    "        # need to add to the original initializer\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "        rho_W3_prev = np.zeros(self.W3.shape) # for momentum\n",
    "        rho_W4_prev = np.zeros(self.W4.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "        self.grad_w4_ = np.zeros(self.epochs)\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5 = self._feedforward(X_data[idx],\n",
    "                                                               self.W1,\n",
    "                                                               self.W2,\n",
    "                                                               self.W3,\n",
    "                                                               self.W4,\n",
    "                                                               self.b1,\n",
    "                                                               self.b2,\n",
    "                                                               self.b3,\n",
    "                                                               self.b4\n",
    "                                                               )\n",
    "\n",
    "                cost = self._cost(A5,Y_enc[:, idx],self.W1,self.W2,self.W3, self.W4)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradW3, gradW4, gradb1, gradb2, gradb3, gradb4 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4, A5=A5,\n",
    "                                                                                                    Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4,\n",
    "                                                                                                    Y_enc=Y_enc[:, idx],\n",
    "                                                                                                    W1=self.W1,W2=self.W2,W3=self.W3,W4=self.W4)\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2, rho_W3, rho_W4 = eta * gradW1, eta * gradW2, eta * gradW3, eta*gradW4\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev)) # update with momentum\n",
    "                self.W4 -= (rho_W4 + (self.alpha * rho_W4_prev)) # update with momentum\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                self.b3 -= eta * gradb3\n",
    "                self.b4 -= eta * gradb4\n",
    "                rho_W1_prev, rho_W2_prev, rho_W3_prev, rho_W4_prev = rho_W1, rho_W2, rho_W3, rho_W4\n",
    "\n",
    "            self.grad_w1_[i] = np.mean(gradW1)\n",
    "            self.grad_w2_[i] = np.mean(gradW2)\n",
    "            self.grad_w3_[i] = np.mean(gradW3)\n",
    "            self.grad_w4_[i] = np.mean(gradW4)\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e55130d",
   "metadata": {},
   "source": [
    "### 4. Adaptive momentum (AdaM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610390b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fourLayer = FourLPMiniBatch(**vals)\n",
    "%time fourLayer.fit(X_train_normal_hot, y_train, print_progress=1, XY_test=(X_test_normal_hot, y_test))\n",
    "plt.plot(abs(fourLayer.grad_w1_[10:]), label='w1')\n",
    "plt.plot(abs(fourLayer.grad_w2_[10:]), label='w2')\n",
    "plt.plot(abs(fourLayer.grad_w3_[10:]), label='w3')\n",
    "plt.plot(abs(fourLayer.grad_w4_[10:]), label='w4')\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print_result(fourLayer,X_train_normal_hot,y_train,X_test_normal_hot,y_test,title=\"Normalization\",color=\"green\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecab38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FiveLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30, n_hidden2 = 25, n_hidden3 = 20, n_hidden4 = 15,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_hidden3 = n_hidden3\n",
    "        self.n_hidden4 = n_hidden4\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_))\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_hidden2 + self.n_hidden))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_hidden2, self.n_hidden))\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_hidden3 + self.n_hidden2))\n",
    "        W3 = np.random.uniform(-init_bound, init_bound,(self.n_hidden3, self.n_hidden2))\n",
    "\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_hidden4 + self.n_hidden3))\n",
    "        W4 = np.random.uniform(-init_bound, init_bound,(self.n_hidden4, self.n_hidden3))\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden4))\n",
    "        W5 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden4))\n",
    "\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        b2 = np.zeros((self.n_hidden2, 1))\n",
    "        b3 = np.zeros((self.n_hidden3, 1))\n",
    "        b4 = np.zeros((self.n_hidden4, 1))\n",
    "        b5 = np.zeros((self.n_output_, 1))\n",
    "\n",
    "        return W1, W2, W3, W4, W5, b1, b2, b3, b4, b5\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2, W3, W4, W5):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2)+ np.mean(W3[:, 1:] ** 2)+ np.mean(W4[:, 1:] ** 2)\n",
    "                                       + np.mean(W5[:, 1:] ** 2))\n",
    "\n",
    "    def _cost(self,A6,Y_enc,W1,W2,W3,W4,W5):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A6)+(1-Y_enc)*np.log(1-A6))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4, W5)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _feedforward(self, X, W1, W2, W3, W4, W5, b1, b2, b3, b4, b5):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs\n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        Z3 = W3 @ A3 + b3\n",
    "        A4 = self._sigmoid(Z3)\n",
    "        Z4 = W4 @ A4 + b4\n",
    "        A5 = self._sigmoid(Z4)\n",
    "        Z5 = W5 @ A5 + b5\n",
    "        A6 = self._sigmoid(Z5)\n",
    "        return A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A6\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, A4, A5, A6, Z1, Z2, Z3, Z4, Z5, Y_enc, W1, W2, W3, W4, W5):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V5 = -2*(Y_enc-A6)*A6*(1-A6)\n",
    "        V4 = A5*(1-A5)*(W5.T @ V5)\n",
    "        V3 = A4*(1-A4)*(W4.T @ V4)\n",
    "        V2 = A3*(1-A3)*(W3.T @ V3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "\n",
    "        gradW5 = V5 @ A5.T\n",
    "        gradW4 = V4 @ A4.T\n",
    "        gradW3 = V3 @ A3.T\n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "\n",
    "        gradb5 = np.sum(V5, axis=1).reshape((-1,1))\n",
    "        gradb4 = np.sum(V4, axis=1).reshape((-1,1))\n",
    "        gradb3 = np.sum(V3, axis=1).reshape((-1,1))\n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C\n",
    "        gradW2 += W2 * self.l2_C\n",
    "        gradW3 += W3 * self.l2_C\n",
    "        gradW4 += W4 * self.l2_C\n",
    "        gradW5 += W5 * self.l2_C\n",
    "\n",
    "        return gradW1, gradW2, gradW3, gradW4, gradW5, gradb1, gradb2, gradb3, gradb4, gradb5\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, _, _, _, _, _, _,A6 = self._feedforward(X, self.W1, self.W2, self.W3, self.W4, self.W5, self.b1, self.b2, self.b3, self.b4, self.b5)\n",
    "        y_pred = np.argmax(A6, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcdf01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FiveLPMiniBatch(FiveLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.1,\n",
    "                 decrease_iter = 10, shuffle=True,\n",
    "                 minibatches=1, **kwds):\n",
    "        # need to add to the original initializer\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.decrease_iter = decrease_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.W3, self.W4, self.W5, self.b1, self.b2, self.b3, self.b4, self.b5 = self._initialize_weights()\n",
    "\n",
    "        # start momentum at zero for previous updates\n",
    "        rho_W1_prev = np.zeros(self.W1.shape) # for momentum\n",
    "        rho_W2_prev = np.zeros(self.W2.shape) # for momentum\n",
    "        rho_W3_prev = np.zeros(self.W3.shape) # for momentum\n",
    "        rho_W4_prev = np.zeros(self.W4.shape) # for momentum\n",
    "        rho_W5_prev = np.zeros(self.W5.shape) # for momentum\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.grad_w3_ = np.zeros(self.epochs)\n",
    "        self.grad_w4_ = np.zeros(self.epochs)\n",
    "        self.grad_w5_ = np.zeros(self.epochs)\n",
    "        \n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            self.val_cost_ = []\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # decrease at certain epochs\n",
    "            eta = self.eta * self.decrease_const**(np.floor(i/self.decrease_iter))\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A6 = self._feedforward(X_data[idx],\n",
    "                                                               self.W1,\n",
    "                                                               self.W2,\n",
    "                                                               self.W3,\n",
    "                                                               self.W4,\n",
    "                                                               self.W5,\n",
    "                                                               self.b1,\n",
    "                                                               self.b2,\n",
    "                                                               self.b3,\n",
    "                                                               self.b4,\n",
    "                                                               self.b5                \n",
    "                                                               )\n",
    "\n",
    "                cost = self._cost(A6,Y_enc[:, idx],self.W1,self.W2,self.W3, self.W4, self.W5)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradW1, gradW2, gradW3, gradW4, gradW5, gradb1, gradb2, gradb3, gradb4, gradb5 = self._get_gradient(A1=A1, A2=A2, A3=A3, A4=A4, A5=A5,\n",
    "                                                                                                    A6=A6, Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4, Z5=Z5,\n",
    "                                                                                                    Y_enc=Y_enc[:, idx],\n",
    "                                                                                                    W1=self.W1,W2=self.W2,W3=self.W3,W4=self.W4,W5=self.W5)\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2, rho_W3, rho_W4, rho_W5 = eta * gradW1, eta * gradW2, eta * gradW3, eta*gradW4, eta*gradW5\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev)) # update with momentum\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev)) # update with momentum\n",
    "                self.W3 -= (rho_W3 + (self.alpha * rho_W3_prev)) # update with momentum\n",
    "                self.W4 -= (rho_W4 + (self.alpha * rho_W4_prev)) # update with momentum\n",
    "                self.W5 -= (rho_W5 + (self.alpha * rho_W5_prev)) # update with momentum\n",
    "                self.b1 -= eta * gradb1\n",
    "                self.b2 -= eta * gradb2\n",
    "                self.b3 -= eta * gradb3\n",
    "                self.b4 -= eta * gradb4\n",
    "                self.b5 -= eta * gradb5\n",
    "                rho_W1_prev, rho_W2_prev, rho_W3_prev, rho_W4_prev, rho_W5_prev = rho_W1, rho_W2, rho_W3, rho_W4, rho_W5\n",
    "\n",
    "            self.grad_w1_[i] = np.mean(gradW1)\n",
    "            self.grad_w2_[i] = np.mean(gradW2)\n",
    "            self.grad_w3_[i] = np.mean(gradW3)\n",
    "            self.grad_w4_[i] = np.mean(gradW4)\n",
    "            self.grad_w5_[i] = np.mean(gradW5)\n",
    "            self.cost_.append(np.mean(mini_cost))\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                yhat = self.predict(X_test)\n",
    "                self.val_score_.append(accuracy_score(y_test,yhat))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6a8e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fiveLayer = FiveLPMiniBatch(**vals)\n",
    "%time fiveLayer.fit(X_train_normal_hot, y_train, print_progress=1, XY_test=(X_test_normal_hot, y_test))\n",
    "\n",
    "plt.plot(abs(fiveLayer.grad_w1_[10:]), label='w1')\n",
    "plt.plot(abs(fiveLayer.grad_w2_[10:]), label='w2')\n",
    "plt.plot(abs(fiveLayer.grad_w3_[10:]), label='w3')\n",
    "plt.plot(abs(fiveLayer.grad_w4_[10:]), label='w4')\n",
    "plt.plot(abs(fiveLayer.grad_w5_[10:]), label='w5')\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_result(fiveLayer,X_train_normal_hot,y_train,X_test_normal_hot,y_test,title=\"Normalization\",color=\"green\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}