{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe86ed7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab Assignment Five: Wide and Deep Network Architectures\n",
    " \n",
    "\n",
    "#### Everett Cienkus, Blake Miller, Colin Weil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adba831",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb0229",
   "metadata": {},
   "source": [
    "#### 1.1 Define and Prepare Class Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1d91d",
   "metadata": {},
   "source": [
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data into memory and save it to a pandas data frame.\n",
    "df = pd.read_csv('census_dataset/acs2017_census_tract_data.csv')\n",
    "\n",
    "# Remove any observations that having missing data.\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode any string data as integers for now. \n",
    "le = LabelEncoder()\n",
    "df['State'] = le.fit_transform(df['State'])\n",
    "df['County'] = le.fit_transform(df['County'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4c590",
   "metadata": {},
   "source": [
    "#### 1.2 Combine into Cross-Product Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68d773",
   "metadata": {},
   "source": [
    "Identify groups of features in your data that should be combined into cross-product features. Provide justification for why these features should be crossed (or why some features should not be crossed). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac6fafd",
   "metadata": {},
   "source": [
    "#### 1.3 Choose Metrics to Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccfad55",
   "metadata": {},
   "source": [
    "Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fef24f",
   "metadata": {},
   "source": [
    "#### 1.4 Choose Method for Dividing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8afae",
   "metadata": {},
   "source": [
    "Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Argue why your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8ed74",
   "metadata": {},
   "source": [
    "Since our dataset is over 50,000, it is okay to use 80/20 split. EXPAND ON THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701fbe1a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6d2f1",
   "metadata": {},
   "source": [
    "#### 2.1 Create Three Combined Wide and Deep Netowkrs using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c9e0a",
   "metadata": {},
   "source": [
    "Create at least three combined wide and deep networks to classify your data using Keras. Visualize the performance of the network on the training data and validation data in the same plot versus the training iterations. Note: use the \"history\" return parameter that is part of Keras \"fit\" function to easily access this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42a036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "990d48fe",
   "metadata": {},
   "source": [
    "#### 2.2 Investigate Performance by Altering the Number of Layers in the Deep Branch of the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2a928",
   "metadata": {},
   "source": [
    "Investigate generalization performance by altering the number of layers in the deep branch of the network. Try at least two different number of layers. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab to select the number of layers that performs superiorly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e991851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce1d3a88",
   "metadata": {},
   "source": [
    "#### 2.3 Investigate Performance of the Best Wide and Deep Network to Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed9555",
   "metadata": {},
   "source": [
    "Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP). Alternatively, you can compare to a network without the wide branch (i.e., just the deep network). For classification tasks, compare using the receiver operating characteristic and area under the curve. For regression tasks, use Bland-Altman plots and residual variance calculations.  Use proper statistical methods to compare the performance of different models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9fc853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be73ef7e",
   "metadata": {},
   "source": [
    "### 3. Capturing the Embedding Weights from the Deep Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7912e89",
   "metadata": {},
   "source": [
    "Capture the embedding weights from the deep network and (if needed) perform dimensionality reduction on the output of these embedding layers (only if needed). That is, pass the observations into the network, save the embedded weights (called embeddings), and then perform  dimensionality reduction in order to visualize results. Visualize and explain any clusters in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957af2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
