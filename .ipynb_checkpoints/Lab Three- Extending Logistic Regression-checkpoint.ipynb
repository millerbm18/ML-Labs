{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe86ed7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab Three: Extending Logistic Regression\n",
    " \n",
    "\n",
    "#### Everett Cienkus, Blake Miller, Colin Weil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adba831",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Preparation and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b870a3b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.1 Business Case\n",
    "\n",
    "   This data set is a collection of data on the specs of light seen in the night sky. This data was collected by the Sloan Digital Sky Survey (SDSS) is a collection of 100,000 observations. There are 17 feature columns comprised of different spectral characteristics and one classification column that classifies the member of the dataset as either a star, galaxy, or quasar. This data is collected using large-scale telescopes owned by the SDSS and their observations are recorded by the employees. The SDSS is an organization that is funded by the Alfred P. Sloan Foundation, The National Science Foundation, and The US Department of Energy Office of Science meaning the collection of this data is also supported and endorsed by the government. \n",
    "    \n",
    "   The classification task for this dataset would be to figure out if the member of the dataset is a star, galaxy, or quasar based on the 17 specter characteristics. According to the SDSS, they have been able to create a 3D map of 1/3 of the night sky with 2/3 of the sky left to map. They have accomplished a great amount since they started, but the classification process takes a long time. After collecting all the data from the telescope, they need to back over the data to try and classify what they observed. With our classification algorithm, they could collect the data and get a classification while still working within the telescope. Not only would this cut down on the time spent classifying the data themselves, but they could also draw further conclusions from a cluster they are observing because they know what it is. With the SDSSâ€™s funding from the government, there is a desire to get all the night sky mapped and the investment into this mapping is already established. This classification task would be beneficial to the SDSS and could be priced at the money saved from not needing to pay the team of people for the hours they would have spent classifying the lights in the sky. They would still need to collect all of the data, but the classification model would do the rest. \n",
    "\n",
    "   There are a number of different classification models that have been used to analyze and classify different stars in the Morgan-Keenan star classification dataset. This dataset consists of different characteristics of a star like effective temperature and luminosity to classify the type of star. All these different models worked well on the dataset, but they found that they could achieve the highest accuracy when they combined the models. They claimed that creating a hybrid of all the models allowed the classification to be more versatile and achieve a success rate of 80%. This data set is similar to the one we are using to the point where we can use their success rate as a benchmark for our own. To prove that our classification method is ahead of others and at the forefront of stellar classification we need to have a success rate above 80%. This is the rate that would convince the SDSS that our classification model is the best one to help their company efficiency finish their 3d mapping the last 2/3 of the night sky.\n",
    "    \n",
    "Information about the specifics of this dataset and more can be found at the following links. \n",
    "\n",
    "https://www.kaggle.com/datasets/fedesoriano/stellar-classification-dataset-sdss17 \n",
    "\n",
    "https://www.sdss.org/\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7517012/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f269961",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results. For example, would the model be deployed or used mostly for offline analysis? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7bcdd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2 Preparation of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3abd257b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   alpha     100000 non-null  float64\n",
      " 1   delta     100000 non-null  float64\n",
      " 2   u         100000 non-null  float64\n",
      " 3   g         100000 non-null  float64\n",
      " 4   r         100000 non-null  float64\n",
      " 5   i         100000 non-null  float64\n",
      " 6   z         100000 non-null  float64\n",
      " 7   redshift  100000 non-null  float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 6.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Series name: class\n",
      "Non-Null Count   Dtype \n",
      "--------------   ----- \n",
      "100000 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define and prepare your class variables.\n",
    "df = pd.read_csv('star_dataset/star_classification.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "X = df.drop(columns = ['obj_ID','run_ID','rerun_ID','field_ID','spec_obj_ID', 'MJD', 'class','plate', 'fiber_ID','cam_col' ])\n",
    "#X = df.drop(columns = ['obj_ID','run_ID','rerun_ID','field_ID','spec_obj_ID', 'MJD', 'class'])\n",
    "y = df['class']\n",
    "\n",
    "# Use proper variable representations (int, float, one-hot, etc.).\n",
    "# Use pre-processing methods (as needed) for dimensionality reduction, \n",
    "# scaling, etc. Remove variables that are not needed/useful for the analysis. \n",
    "# Describe the final dataset that is used for classification/regression\n",
    "display(X.info())\n",
    "display(y.info())\n",
    "# (include a description of any newly formed variables you created).\n",
    "# MAKE SURE TO NORMALIZE VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e0f13",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cf2f0c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.3 Division of Trainig and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92732431",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['GALAXY' 47545]\n",
      " ['QSO' 15155]\n",
      " ['STAR' 17300]]\n"
     ]
    }
   ],
   "source": [
    "# Divide your data into training and testing data using an 80% training \n",
    "# and 20% testing split. Use the cross validation modules that are part \n",
    "# of scikit-learn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, train_size=0.8)\n",
    "\n",
    "unique_ytrain, counts_ytrain = np.unique(y_train, return_counts=True)\n",
    "print(np.asarray((unique_ytrain, counts_ytrain)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c7c05",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Argue \"for\" or \"against\" splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a6c14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b8acc2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1 One-Versus-All Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70dd9dbe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def regularize(w, c, reg):\n",
    "    if reg == 'L2':\n",
    "        return -2 * w[1:] * c\n",
    "    if reg == 'L1':\n",
    "        return  - w[1:] / abs(w[1:]) * c\n",
    "    if reg == \"both\":\n",
    "        return regularize(w, c, 'L2') + regularize(w, c, 'L1')\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001, reg =\"L2\"):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.reg = reg\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "\n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "\n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += regularize(w= self.w_, c = self.C, reg=self.reg)\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "\n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate\n",
    "            # add bacause maximizing\n",
    "\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += regularize(w= self.w_, c = self.C, reg=self.reg)\n",
    "\n",
    "        return gradient\n",
    "\n",
    "# for this, we won't perform our own BFGS implementation\n",
    "# (it takes a fair amount of code and understanding, which we haven't setup yet)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs # maybe the most common bfgs algorithm in the world\n",
    "from numpy import ma\n",
    "np.seterr(invalid='ignore')\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "        return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(w**2)\n",
    "        #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    def objective_gradient(self,w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += regularize(w= w, c = C, reg=self.reg)\n",
    "        return -gradient\n",
    "\n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "\n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "\n",
    "\n",
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20,\n",
    "                 C=0.0001,\n",
    "                 solver=BFGSBinaryLogisticRegression, reg =\"L2\"):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.classifiers_ = []\n",
    "        self.reg = reg\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = np.array(y==yval).astype(int) # create a binary problem\n",
    "\n",
    "            # train the binary classifier for this class\n",
    "\n",
    "            hblr = self.solver(eta=self.eta,iterations=self.iters,C=self.C, reg=self.reg)\n",
    "            hblr.fit(X,y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "\n",
    "        return np.hstack(probs) # make into single matrix\n",
    "\n",
    "    def predict(self,X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X),axis=1)] # take argmax along row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51891e50",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Steepest Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "665a6108",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.60185\n",
      "[['GALAXY' 19588]\n",
      " ['QSO' 412]]\n",
      "[['GALAXY' 11900]\n",
      " ['QSO' 3806]\n",
      " ['STAR' 4294]]\n",
      "CPU times: user 17.4 s, sys: 4.22 s, total: 21.6 s\n",
      "Wall time: 5.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.05,\n",
    "                                  iterations=1000,\n",
    "                                  C=0.0001,\n",
    "                                  solver=BinaryLogisticRegression,\n",
    "                                  reg=\"L2\"\n",
    "                                  )\n",
    "#np.hstack((np.ones((X.shape[0],1)),X))\n",
    "lr.fit(X_train,y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "unique_yhat, counts_yhat = np.unique(yhat, return_counts=True)\n",
    "unique_y, counts_y = np.unique(y_test, return_counts=True)\n",
    "print(np.asarray((unique_yhat, counts_yhat)).T)\n",
    "print(np.asarray((unique_y, counts_y)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9b6a4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stochastic gradient Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0194ec6e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.215\n",
      "[['GALAXY' 1003]\n",
      " ['QSO' 18997]]\n",
      "[['GALAXY' 11900]\n",
      " ['QSO' 3806]\n",
      " ['STAR' 4294]]\n",
      "CPU times: user 422 ms, sys: 148 ms, total: 571 ms\n",
      "Wall time: 128 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.05,\n",
    "                                  iterations=1000,\n",
    "                                  C=0.0001,\n",
    "                                  solver=StochasticLogisticRegression,\n",
    "                                  reg=\"none\"\n",
    "                                  )\n",
    "#np.hstack((np.ones((X.shape[0],1)),X))\n",
    "lr.fit(X_train,y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "unique_yhat, counts_yhat = np.unique(yhat, return_counts=True)\n",
    "unique_y, counts_y = np.unique(y_test, return_counts=True)\n",
    "print(np.asarray((unique_yhat, counts_yhat)).T)\n",
    "print(np.asarray((unique_y, counts_y)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770393e7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ac4c404",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.85905\n",
      "[['GALAXY' 12518]\n",
      " ['QSO' 3667]\n",
      " ['STAR' 3815]]\n",
      "[['GALAXY' 11900]\n",
      " ['QSO' 3806]\n",
      " ['STAR' 4294]]\n",
      "CPU times: user 2.76 s, sys: 834 ms, total: 3.59 s\n",
      "Wall time: 864 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=1,\n",
    "                                  iterations=10,\n",
    "                                  C=0.00001,\n",
    "                                  solver=BFGSBinaryLogisticRegression,\n",
    "                                  reg='L2'\n",
    "                                  )\n",
    "#np.hstack((np.ones((X.shape[0],1)),X))\n",
    "lr.fit(X_train,y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "unique_yhat, counts_yhat = np.unique(yhat, return_counts=True)\n",
    "unique_y, counts_y = np.unique(y_test, return_counts=True)\n",
    "print(np.asarray((unique_yhat, counts_yhat)).T)\n",
    "print(np.asarray((unique_y, counts_y)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567611e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.2 Training Classifier for Good Generalization Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a23b1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of C= 0.1  : 0.74795\n",
      "Accuracy of C= 0.010000000000000002  : 0.83905\n"
     ]
    }
   ],
   "source": [
    "c = []\n",
    "acc = []\n",
    "for i in range(8):\n",
    "    lr = MultiClassLogisticRegression(eta=.5,\n",
    "                                      iterations=45,\n",
    "                                      C=.1*(10**(-i)),\n",
    "                                      solver=BFGSBinaryLogisticRegression,\n",
    "                                      reg='L2'\n",
    "                                      )\n",
    "    lr.fit(X_train,y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    c.append(.01*(10**(-i)))\n",
    "    acc.append(accuracy_score(y_test,yhat))\n",
    "    print('Accuracy of C=',.1*(10**(-i)),\" :\",accuracy_score(y_test,yhat))\n",
    "# df = pd.DataFrame(data=[acc],columns=[c] )\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961464ac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?\n",
    "\n",
    "There is definitely data snooping involved with this method. When creating the model and testing the accuracy we chose the eta, iterations, and C value. This means that we have tailored the model to work in favor of the dataset that we are working with. If a different dataset were to be used then we would get a different, less accurate performance because the model wouldn't be tailored to the new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c886f127",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.3 Comparing Best Performing Procedure to Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a7b946",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='liblinear') # all params default\n",
    "\n",
    "lr_sk.fit(X,y)\n",
    "#print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9a31f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the performance differences in terms of training time and classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd45fa5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Discuss the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7640c7b4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da079c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?\n",
    "\n",
    "Out of all of the logistic regression implemenations, we would deploy our BFGS model using the parameters that gave the highest percentage. These paramaters returned an accuracy of 94.41% which is very high compared to the other classification algorithms that have been used before us. This accuracy will allow the people that are classifying the stars to spend less time actually classifying themselves and in return will cost less money to pay them to do so. Although our timing is slightly higher than the Scikit-learn implemenation, we belive that the accuracy advantage makes a big difference because although it is only around 1%, this is 1% of a 100,000 overvations (or more in the future). This process already takes a long time and now since a computer would be classiyfing and the manual work would not have to be compensated, a higher accuracy is more important than timing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f23862",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4. BFGS (Can change but thought this would be better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c40744",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Implementation of BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977a1cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SciPY BFGS\n",
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=1,\n",
    "                                  iterations=10,\n",
    "                                  C=0.00001,\n",
    "                                  solver=BFGSBinaryLogisticRegression,\n",
    "                                  reg='L2'\n",
    "                                  )\n",
    "#np.hstack((np.ones((X.shape[0],1)),X))\n",
    "lr.fit(X_train,y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "unique_yhat, counts_yhat = np.unique(yhat, return_counts=True)\n",
    "unique_y, counts_y = np.unique(y_test, return_counts=True)\n",
    "print(np.asarray((unique_yhat, counts_yhat)).T)\n",
    "print(np.asarray((unique_y, counts_y)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7456c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
