{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe86ed7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab Assignment Five: Wide and Deep Network Architectures\n",
    " \n",
    "\n",
    "#### Everett Cienkus, Blake Miller, Colin Weil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adba831",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb0229",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.1 Define and Prepare Class Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb04a66",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdd1d91d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data from https://www.kaggle.com/datasets/arashnic/hr-ana\n",
    "\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "This data set is a collection of different types of characteristics of over 50,000 employees who work for large MNCs (Multinational Corporations). These characteristics are paired with whether or not the employee was recommended to be promoted after being evaluated. There are 13 feature columns comprised of different characteristics and accomplishments of the employees, but we did not use all of the features. We decided not to use age or gender because we did not believe that either of these characteristics should be considered when evaluating an individual for a promotion. The importance of this data set is to cut down on time taken to evaluate countless employees for a position. Our algorithm will be able to reduce the employees needed for evaluation so that those giving the promotion can use their valuable time to be productive in other ways. Methods like this have already been used by companies to sort through an abundance on resumes and produce a smaller list of best fit candidates for a certain job. No new variables were created for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9a2de0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_id</th>\n",
       "      <th>department</th>\n",
       "      <th>region</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>recruitment_channel</th>\n",
       "      <th>no_of_trainings</th>\n",
       "      <th>age</th>\n",
       "      <th>previous_year_rating</th>\n",
       "      <th>length_of_service</th>\n",
       "      <th>awards_won?</th>\n",
       "      <th>avg_training_score</th>\n",
       "      <th>is_promoted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15840</th>\n",
       "      <td>71539</td>\n",
       "      <td>Technology</td>\n",
       "      <td>region_13</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43612</th>\n",
       "      <td>43310</td>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_13</td>\n",
       "      <td>Master's &amp; above</td>\n",
       "      <td>m</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30332</th>\n",
       "      <td>13162</td>\n",
       "      <td>HR</td>\n",
       "      <td>region_26</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10776</th>\n",
       "      <td>57708</td>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_15</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>71268</td>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_28</td>\n",
       "      <td>Master's &amp; above</td>\n",
       "      <td>f</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6176</th>\n",
       "      <td>52563</td>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_29</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>68124</td>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_13</td>\n",
       "      <td>Master's &amp; above</td>\n",
       "      <td>m</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48794</th>\n",
       "      <td>16746</td>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_22</td>\n",
       "      <td>Master's &amp; above</td>\n",
       "      <td>f</td>\n",
       "      <td>other</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30799</th>\n",
       "      <td>53107</td>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_25</td>\n",
       "      <td>Master's &amp; above</td>\n",
       "      <td>m</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>27470</td>\n",
       "      <td>Legal</td>\n",
       "      <td>region_34</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9732 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       employee_id         department     region         education gender  \\\n",
       "15840        71539         Technology  region_13        Bachelor's      m   \n",
       "43612        43310  Sales & Marketing  region_13  Master's & above      m   \n",
       "30332        13162                 HR  region_26        Bachelor's      m   \n",
       "10776        57708  Sales & Marketing  region_15        Bachelor's      m   \n",
       "5075         71268  Sales & Marketing  region_28  Master's & above      f   \n",
       "...            ...                ...        ...               ...    ...   \n",
       "6176         52563  Sales & Marketing  region_29        Bachelor's      m   \n",
       "2297         68124  Sales & Marketing  region_13  Master's & above      m   \n",
       "48794        16746  Sales & Marketing  region_22  Master's & above      f   \n",
       "30799        53107  Sales & Marketing  region_25  Master's & above      m   \n",
       "939          27470              Legal  region_34        Bachelor's      m   \n",
       "\n",
       "      recruitment_channel  no_of_trainings  age  previous_year_rating  \\\n",
       "15840            sourcing                1   45                   1.0   \n",
       "43612               other                1   39                   4.0   \n",
       "30332            sourcing                1   30                   3.0   \n",
       "10776               other                1   33                   1.0   \n",
       "5075                other                1   37                   5.0   \n",
       "...                   ...              ...  ...                   ...   \n",
       "6176                other                1   33                   3.0   \n",
       "2297             sourcing                1   36                   3.0   \n",
       "48794               other                2   32                   4.0   \n",
       "30799            sourcing                1   37                   5.0   \n",
       "939                 other                1   27                   3.0   \n",
       "\n",
       "       length_of_service  awards_won?  avg_training_score  is_promoted  \n",
       "15840                 17            0                  76            0  \n",
       "43612                  3            0                  52            0  \n",
       "30332                  4            0                  52            0  \n",
       "10776                  7            0                  53            0  \n",
       "5075                   4            0                  49            0  \n",
       "...                  ...          ...                 ...          ...  \n",
       "6176                   6            1                  49            0  \n",
       "2297                   4            0                  47            0  \n",
       "48794                  7            1                  48            0  \n",
       "30799                  2            0                  49            0  \n",
       "939                    3            0                  63            0  \n",
       "\n",
       "[9732 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load the data into memory and save it to a pandas data frame.\n",
    "df = pd.read_csv('promotion_dataset/train.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "df_train, df_test = train_test_split(df,train_size=0.8)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acfb1ce7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the following 11 features:\n",
      "[   'department_int',\n",
      "    'region_int',\n",
      "    'education_int',\n",
      "    'gender_int',\n",
      "    'recruitment_channel_int',\n",
      "    'no_of_trainings',\n",
      "    'previous_year_rating',\n",
      "    'length_of_service',\n",
      "    'awards_won?',\n",
      "    'avg_training_score',\n",
      "    'age']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# ========================================================\n",
    "# define objects that can encode each variable as integer\n",
    "encoders = dict() # save each encoder in dictionary\n",
    "categorical_headers = ['department','region','education','gender','recruitment_channel']\n",
    "# train all encoders\n",
    "for col in categorical_headers:\n",
    "    df_train[col] = df_train[col].str.strip()\n",
    "    df_test[col] = df_test[col].str.strip()\n",
    "    encoders[col] = LabelEncoder() # save the encoder\n",
    "    df_train[col+'_int'] = encoders[col].fit_transform(df_train[col])\n",
    "    df_test[col+'_int'] = encoders[col].transform(df_test[col])\n",
    "# ========================================================\n",
    "# scale the numeric, continuous variables\n",
    "numeric_headers = ['no_of_trainings', 'previous_year_rating', 'length_of_service', 'awards_won?', 'avg_training_score','age']\n",
    "ss = StandardScaler()\n",
    "df_train[numeric_headers] = ss.fit_transform(df_train[numeric_headers].values)\n",
    "df_test[numeric_headers] = ss.transform(df_test[numeric_headers].values)\n",
    "\n",
    "\n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "\n",
    "feature_columns = categorical_headers_ints+numeric_headers\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "print(f\"We will use the following {len(feature_columns)} features:\")\n",
    "pp.pprint(feature_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4c590",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2 Combine into Cross-Product Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656b605",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One of the crosses we decided to use is crossing the department column and the education column. These would be good to combine because certain departments might have different promotion rates as well as education will be very important to some departments and if someone should be promoted inside of them. \n",
    "\n",
    "We also chose to combine the recruitment channel and education columns. This would be a good idea because the way someone enters the compnay tells a lot about how they will progress throughout the company. Pairing this concept with education may bring some strong results as people who get recruited with the same education type would be expected to have similar career paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91dab1af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department has 9 unique values:\n",
      "['Sales & Marketing' 'Analytics' 'HR' 'Technology' 'Procurement'\n",
      " 'Operations' 'R&D' 'Finance' 'Legal']\n",
      "region has 34 unique values:\n",
      "['region_27' 'region_2' 'region_28' 'region_15' 'region_19' 'region_7'\n",
      " 'region_22' 'region_26' 'region_25' 'region_23' 'region_14' 'region_31'\n",
      " 'region_5' 'region_30' 'region_3' 'region_17' 'region_32' 'region_13'\n",
      " 'region_20' 'region_16' 'region_11' 'region_10' 'region_6' 'region_29'\n",
      " 'region_9' 'region_12' 'region_4' 'region_24' 'region_8' 'region_1'\n",
      " 'region_33' 'region_21' 'region_34' 'region_18']\n",
      "education has 3 unique values:\n",
      "[\"Bachelor's\" \"Master's & above\" 'Below Secondary']\n",
      "gender has 2 unique values:\n",
      "['m' 'f']\n",
      "recruitment_channel has 3 unique values:\n",
      "['other' 'sourcing' 'referred']\n"
     ]
    }
   ],
   "source": [
    "for col in categorical_headers:\n",
    "    vals = df_train[col].unique()\n",
    "    print(col,'has', len(vals), 'unique values:')\n",
    "    print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8209aae2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['department_education', 'recruitment_channel_education', 'department_region']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_columns = [\n",
    "    ['department','education'],\n",
    "    ['recruitment_channel','education'],\n",
    "    ['department', 'region']\n",
    "]\n",
    "\n",
    "# cross each set of columns in the list above\n",
    "cross_col_df_names = []\n",
    "for cols_list in cross_columns:\n",
    "    # encode as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "\n",
    "    # 1. create crossed labels by join operation\n",
    "    X_crossed_train = df_train[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "    X_crossed_test = df_test[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "    # get a nice name for this new crossed column\n",
    "    cross_col_name = '_'.join(cols_list)\n",
    "\n",
    "    # 2. encode as integers, stacking all possibilities\n",
    "    enc.fit(np.hstack((X_crossed_train.to_numpy(),  X_crossed_test.to_numpy())))\n",
    "\n",
    "    # 3. Save into dataframe with new name\n",
    "    df_train[cross_col_name] = enc.transform(X_crossed_train)\n",
    "    df_test[cross_col_name] = enc.transform(X_crossed_test)\n",
    "\n",
    "    # keep track of the new names of the crossed columns\n",
    "    cross_col_df_names.append(cross_col_name)\n",
    "\n",
    "cross_col_df_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac6fafd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.3 Choose Metrics to Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccfad55",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will be using precision as our metric to evaluate our algorithms' performance. This metric is appropriate for our dataset becuase precision reduces false positiives and the worst case for the prediction that we will be mkaing would be to promote someone that does not qualify or deserve to be promoted. In addition, many times in the workplace people who are qualified do not get the promotion, so if our algorithm mirrors the real life action and does not classify someone who is qualified to get promotion, this is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fef24f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.4 Choose Method for Dividing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aad2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train = df_train[feature_columns].to_numpy()\n",
    "X_test = df_test[feature_columns].to_numpy()\n",
    "\n",
    "y_train = df_train['is_promoted'].to_numpy()\n",
    "y_test = df_test['is_promoted'].to_numpy()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8ed74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since our dataset is over 50,000, it is okay to use 80/20 split according to the Larson Rule (we felt like this was a good name). When splitting our data set, it is important to ensure there is an even distribution positive outcomes and negative outcomes in both the testing and training data. This allows the data to be less biased, allowing the algorithm to train with a diverse dataset. The 80/20 rule works in this case bacause the large data set almost garentees that there will be diverse data because the set should contains multiple different combinations of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701fbe1a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6d2f1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1 Create Three Combined Wide and Deep Netowkrs using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c9e0a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create at least three combined wide and deep networks to classify your data using Keras. Visualize the performance of the network on the training data and validation data in the same plot versus the training iterations. Note: use the \"history\" return parameter that is part of Keras \"fit\" function to easily access this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da2fd1c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import concatenate\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e23325d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get crossed columns\n",
    "X_train_crossed = df_train[cross_col_df_names].to_numpy()\n",
    "X_test_crossed = df_test[cross_col_df_names].to_numpy()\n",
    "# save categorical features\n",
    "X_train_cat = df_train[categorical_headers_ints].to_numpy()\n",
    "X_test_cat = df_test[categorical_headers_ints].to_numpy()\n",
    "# and save off the numeric features\n",
    "X_train_num =  df_train[numeric_headers].to_numpy()\n",
    "X_test_num =  df_test[numeric_headers].to_numpy()\n",
    "\n",
    "# we need to create separate lists for each branch\n",
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "\n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "\n",
    "\n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "\n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "\n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "\n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "\n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "\n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "\n",
    "# NUMERIC DATA INPUT\n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "\n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "\n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model1 = Model(inputs=[input_crossed,input_cat,input_num],\n",
    "              outputs=final_branch)\n",
    "\n",
    "model1.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[tf.keras.metrics.Precision()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd915ec1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 20:52:52.412520: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3893/3893 [==============================] - 3s 596us/step - loss: 0.0951 - precision: 0.1023 - val_loss: 0.0765 - val_precision: 0.0000e+00\n",
      "Epoch 2/20\n",
      "3893/3893 [==============================] - 2s 552us/step - loss: 0.0750 - precision: 0.7475 - val_loss: 0.0718 - val_precision: 0.7273\n",
      "Epoch 3/20\n",
      "3893/3893 [==============================] - 2s 556us/step - loss: 0.0724 - precision: 0.7107 - val_loss: 0.0705 - val_precision: 0.6418\n",
      "Epoch 4/20\n",
      "3893/3893 [==============================] - 2s 557us/step - loss: 0.0716 - precision: 0.6955 - val_loss: 0.0700 - val_precision: 0.7558\n",
      "Epoch 5/20\n",
      "3893/3893 [==============================] - 2s 556us/step - loss: 0.0712 - precision: 0.7077 - val_loss: 0.0696 - val_precision: 0.6847\n",
      "Epoch 6/20\n",
      "3893/3893 [==============================] - 2s 562us/step - loss: 0.0709 - precision: 0.6900 - val_loss: 0.0694 - val_precision: 0.6923\n",
      "Epoch 7/20\n",
      "3893/3893 [==============================] - 2s 559us/step - loss: 0.0706 - precision: 0.7265 - val_loss: 0.0690 - val_precision: 0.7009\n",
      "Epoch 8/20\n",
      "3893/3893 [==============================] - 2s 554us/step - loss: 0.0703 - precision: 0.7185 - val_loss: 0.0687 - val_precision: 0.7551\n",
      "Epoch 9/20\n",
      "3893/3893 [==============================] - 2s 564us/step - loss: 0.0698 - precision: 0.7434 - val_loss: 0.0682 - val_precision: 0.7417\n",
      "Epoch 10/20\n",
      "3893/3893 [==============================] - 2s 562us/step - loss: 0.0693 - precision: 0.7646 - val_loss: 0.0677 - val_precision: 0.7630\n",
      "Epoch 11/20\n",
      "3893/3893 [==============================] - 2s 559us/step - loss: 0.0684 - precision: 0.7768 - val_loss: 0.0668 - val_precision: 0.8448\n",
      "Epoch 12/20\n",
      "3893/3893 [==============================] - 2s 550us/step - loss: 0.0671 - precision: 0.8330 - val_loss: 0.0655 - val_precision: 0.8014\n",
      "Epoch 13/20\n",
      "3893/3893 [==============================] - 2s 573us/step - loss: 0.0655 - precision: 0.8654 - val_loss: 0.0645 - val_precision: 0.7717\n",
      "Epoch 14/20\n",
      "3893/3893 [==============================] - 2s 568us/step - loss: 0.0636 - precision: 0.8785 - val_loss: 0.0619 - val_precision: 0.9123\n",
      "Epoch 15/20\n",
      "3893/3893 [==============================] - 2s 574us/step - loss: 0.0615 - precision: 0.9085 - val_loss: 0.0599 - val_precision: 0.9590\n",
      "Epoch 16/20\n",
      "3893/3893 [==============================] - 2s 561us/step - loss: 0.0594 - precision: 0.9367 - val_loss: 0.0586 - val_precision: 0.9948\n",
      "Epoch 17/20\n",
      "3893/3893 [==============================] - 2s 559us/step - loss: 0.0576 - precision: 0.9346 - val_loss: 0.0573 - val_precision: 0.9954\n",
      "Epoch 18/20\n",
      "3893/3893 [==============================] - 2s 557us/step - loss: 0.0565 - precision: 0.9368 - val_loss: 0.0560 - val_precision: 0.8878\n",
      "Epoch 19/20\n",
      "3893/3893 [==============================] - 2s 560us/step - loss: 0.0560 - precision: 0.9273 - val_loss: 0.0557 - val_precision: 0.8910\n",
      "Epoch 20/20\n",
      "3893/3893 [==============================] - 2s 559us/step - loss: 0.0556 - precision: 0.9292 - val_loss: 0.0555 - val_precision: 0.8977\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=10,\n",
    "                    verbose=1,\n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a82abbd1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_precision\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['precision'])\n",
    "plt.plot(history.history['val_precision'])\n",
    "plt.title('model precision')\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b508130",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "yhat = np.round(model1.predict([X_test_crossed,X_test_cat,X_test_num])).astype(int)\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))\n",
    "unique_yhat, counts_yhat = np.unique(yhat, return_counts=True)\n",
    "print(\"Y hat\\n\",np.asarray((unique_yhat, counts_yhat)).T)\n",
    "unique_ytest, counts_ytest = np.unique(y_test, return_counts=True)\n",
    "print(\"actual\\n\",np.asarray((unique_ytest, counts_ytest)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d48fe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.2 Investigate Performance by Altering the Number of Layers in the Deep Branch of the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2a928",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Investigate generalization performance by altering the number of layers in the deep branch of the network. Try at least two different number of layers. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab to select the number of layers that performs superiorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5ca24",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we need to create separate lists for each branch\n",
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "\n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "\n",
    "\n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "\n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "\n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = df_train[col].max()+1\n",
    "\n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "\n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "\n",
    "# NUMERIC DATA INPUT\n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "\n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "# 5 layer network\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=40,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=30,activation='relu', name='deep3')(deep_branch)\n",
    "deep_branch = Dense(units=20,activation='relu', name='deep4')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep5')(deep_branch)\n",
    "\n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model2 = Model(inputs=[input_crossed,input_cat,input_num],\n",
    "              outputs=final_branch)\n",
    "\n",
    "model2.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[tf.keras.metrics.Precision()])\n",
    "\n",
    "history = model2.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=10,\n",
    "                    verbose=1,\n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead761ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "yhat = np.round(model2.predict([X_test_crossed,X_test_cat,X_test_num])).astype(int)\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))\n",
    "unique_yhat, counts_yhat = np.unique(yhat, return_counts=True)\n",
    "print(\"Y hat\\n\",np.asarray((unique_yhat, counts_yhat)).T)\n",
    "unique_ytest, counts_ytest = np.unique(y_test, return_counts=True)\n",
    "print(\"actual\\n\",np.asarray((unique_ytest, counts_ytest)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51abe5e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['precision_1'])\n",
    "plt.plot(history.history['val_precision_1'])\n",
    "plt.title('model precision')\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f0821",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.3 Third wide and deep network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99106a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # get new crossed columns\n",
    "# cross_columns = [\n",
    "#     ['department','education'],\n",
    "#     ['recruitment_channel','education'],\n",
    "#     ['department', 'region'],\n",
    "#     ['department', 'education', 'region']\n",
    "# ]\n",
    "# cross_col_df_names = []\n",
    "# for cols_list in cross_columns:\n",
    "#     # encode as ints for the embedding\n",
    "#     enc = LabelEncoder()\n",
    "#     # 1. create crossed labels by join operation\n",
    "#     X_crossed_train = df_train[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "#     X_crossed_test = df_test[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "#\n",
    "#     # get a nice name for this new crossed column\n",
    "#     cross_col_name = '_'.join(cols_list)\n",
    "#\n",
    "#     # 2. encode as integers, stacking all possibilities\n",
    "#     enc.fit(np.hstack((X_crossed_train.to_numpy(),  X_crossed_test.to_numpy())))\n",
    "#\n",
    "#     # 3. Save into dataframe with new name\n",
    "#     df_train[cross_col_name] = enc.transform(X_crossed_train)\n",
    "#     df_test[cross_col_name] = enc.transform(X_crossed_test)\n",
    "#\n",
    "#     # keep track of the new names of the crossed columns\n",
    "#     cross_col_df_names.append(cross_col_name)\n",
    "#\n",
    "# cross_col_df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13761df4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "crossed_outputs = []\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "\n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "\n",
    "\n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "\n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "\n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "\n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "\n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "\n",
    "# NUMERIC DATA INPUT\n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "\n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "# 4 layer deep network\n",
    "deep_branch = Dense(units=30,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=20,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "deep_branch = Dense(units=5,activation='relu', name='deep4')(deep_branch)\n",
    "\n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model3 = Model(inputs=[input_crossed,input_cat,input_num],\n",
    "              outputs=final_branch)\n",
    "\n",
    "model3.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[tf.keras.metrics.Precision()])\n",
    "\n",
    "history = model3.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=10,\n",
    "                    verbose=1,\n",
    "                    validation_data = ([X_test_crossed,X_test_cat,X_test_num],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798505fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "yhat = np.round(model3.predict([X_test_crossed,X_test_cat,X_test_num])).astype(int)\n",
    "print(mt.confusion_matrix(y_test,yhat))\n",
    "print(mt.classification_report(y_test,yhat))\n",
    "unique_yhat, counts_yhat = np.unique(yhat, return_counts=True)\n",
    "print(\"Y hat\\n\",np.asarray((unique_yhat, counts_yhat)).T)\n",
    "unique_ytest, counts_ytest = np.unique(y_test, return_counts=True)\n",
    "print(\"actual\\n\",np.asarray((unique_ytest, counts_ytest)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb03bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['precision_2'])\n",
    "plt.plot(history.history['val_precision_2'])\n",
    "plt.title('model precision')\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d3a88",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.4 Investigate Performance of the Best Wide and Deep Network to Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed9555",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP). Alternatively, you can compare to a network without the wide branch (i.e., just the deep network). For classification tasks, compare using the receiver operating characteristic and area under the curve. For regression tasks, use Bland-Altman plots and residual variance calculations.  Use proper statistical methods to compare the performance of different models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9fc853",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's define the architecture for a multi-layer network\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "\n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = max(df_train[col].max(),df_test[col].max())+1\n",
    "\n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "\n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "\n",
    "# NUMERIC DATA INPUT\n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "\n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "# 7 layers now\n",
    "\n",
    "deep_branch = Dense(units=30,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=20,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "deep_branch = Dense(units=5,activation='relu', name='deep4')(deep_branch)\n",
    "\n",
    "# merge the deep and wide branch\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(deep_branch)\n",
    "\n",
    "MLPModel = Model(inputs=[input_cat,input_num],\n",
    "              outputs=final_branch)\n",
    "MLPModel.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[tf.keras.metrics.Precision()])\n",
    "history = MLPModel.fit([X_train_cat,X_train_num],\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=10,\n",
    "                    verbose=1,\n",
    "                    validation_data = ([X_test_cat,X_test_num],y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f83e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "yhatMLP = np.round(MLPModel.predict([X_test_cat,X_test_num])).astype(int)\n",
    "print(mt.confusion_matrix(y_test,yhatMLP))\n",
    "print(mt.classification_report(y_test,yhatMLP))\n",
    "unique_yhat, counts_yhat = np.unique(yhatMLP, return_counts=True)\n",
    "print(\"Y hat\\n\",np.asarray((unique_yhat, counts_yhat)).T)\n",
    "unique_ytest, counts_ytest = np.unique(y_test, return_counts=True)\n",
    "print(\"actual\\n\",np.asarray((unique_ytest, counts_ytest)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1b814",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "yhatMLP = MLPModel.predict([X_test_cat,X_test_num]).ravel()\n",
    "fpr_mlp, tpr_mlp, thresholds_keras = roc_curve(y_test, yhatMLP)\n",
    "auc_mlp = auc(fpr_mlp, tpr_mlp)\n",
    "yhat_model1 = model1.predict([X_test_crossed,X_test_cat,X_test_num]).ravel()\n",
    "fpr_model1, tpr_model1, thresholds_keras = roc_curve(y_test, yhat_model1)\n",
    "auc_model1= auc(fpr_model1, tpr_model1)\n",
    "yhat_model2 = model2.predict([X_test_crossed,X_test_cat,X_test_num]).ravel()\n",
    "fpr_model2, tpr_model2, thresholds_keras = roc_curve(y_test, yhat_model2)\n",
    "auc_model2= auc(fpr_model2, tpr_model2)\n",
    "yhat_model3 = model3.predict([X_test_crossed,X_test_cat,X_test_num]).ravel()\n",
    "fpr_model3, tpr_model3, thresholds_keras = roc_curve(y_test, yhat_model3)\n",
    "auc_model3= auc(fpr_model3, tpr_model3)\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_mlp, tpr_mlp, label='MLP (area = {:.3f})'.format(auc_mlp))\n",
    "plt.plot(fpr_model1, tpr_model1, label='Model 1 (area = {:.3f})'.format(auc_model1))\n",
    "plt.plot(fpr_model2, tpr_model2, label='Model 2 (area = {:.3f})'.format(auc_model2))\n",
    "plt.plot(fpr_model3, tpr_model3, label='Model 3 (area = {:.3f})'.format(auc_model3))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb54ab",
   "metadata": {},
   "source": [
    "For each of the models we trained, we get a pretty similar ROC curve "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73ef7e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. Capturing the Embedding Weights from the Deep Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7912e89",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Capture the embedding weights from the deep network and (if needed) perform dimensionality reduction on the output of these embedding layers (only if needed). That is, pass the observations into the network, save the embedded weights (called embeddings), and then perform  dimensionality reduction in order to visualize results. Visualize and explain any clusters in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957af2da",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9e10d",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dept_embeddings = model3.get_layer('department_int_embed').get_weights()[0]\n",
    "print(encoders['department'].inverse_transform([0]))\n",
    "print(len(dept_embeddings))\n",
    "for i in range(len(dept_embeddings)):\n",
    "    print(encoders['department'].inverse_transform([i]))\n",
    "    print(dept_embeddings[i])\n",
    "#words_embeddings = {w:embeddings[idx] for w, idx in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe286028",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d    \n",
    "\n",
    " \n",
    "fig = plt.figure()\n",
    " \n",
    "# syntax for 3-D projection\n",
    "ax = plt.axes(projection ='3d')\n",
    " \n",
    "for i in range(len(dept_embeddings)):\n",
    "    ax.scatter(dept_embeddings[i][0],dept_embeddings[i][1],dept_embeddings[i][2], 'green', label = encoders['department'].inverse_transform([i]) )\n",
    "# plotting\n",
    "#ax.plot3D(x, y, z, 'green')\n",
    "ax.set_title('Learned encodings by department')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b93e3",
   "metadata": {},
   "source": [
    "As we can see from the above plot, we do get some meaningful clusters in the embedding data. On the bottom right side of the plot, we can see that the R&D, Technology, and Analytics departments have clustered embeddings. We can also see that in the center of the plot, the Legal and HR departments   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
