{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe86ed7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab Three: Extending Logistic Regression\n",
    " \n",
    "\n",
    "#### Everett Cienkus, Blake Miller, Colin Weil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adba831",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Preparation and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd1ecc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.1 Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d247dfd4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results. For example, would the model be deployed or used mostly for offline analysis? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8aaca9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2 Preparation of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d776f8b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4870 entries, 0 to 4897\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         4870 non-null   float64\n",
      " 1   volatile acidity      4870 non-null   float64\n",
      " 2   citric acid           4870 non-null   float64\n",
      " 3   residual sugar        4870 non-null   float64\n",
      " 4   chlorides             4870 non-null   float64\n",
      " 5   free sulfur dioxide   4870 non-null   float64\n",
      " 6   total sulfur dioxide  4870 non-null   float64\n",
      " 7   density               4870 non-null   float64\n",
      " 8   pH                    4870 non-null   float64\n",
      " 9   sulphates             4870 non-null   float64\n",
      " 10  alcohol               4870 non-null   float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 456.6 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": "None"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 4870 entries, 0 to 4897\n",
      "Series name: quality\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "4870 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 76.1 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": "None"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define and prepare your class variables.\n",
    "df = pd.read_csv('wine_dataset/winequalityN.csv')\n",
    "df = df[df['type']=='white']\n",
    "df = df.drop(columns = ['type'])\n",
    "df = df.dropna()\n",
    "X = df.drop(columns = ['quality'])\n",
    "y = df['quality']\n",
    "# Use proper variable representations (int, float, one-hot, etc.).\n",
    "# Use pre-processing methods (as needed) for dimensionality reduction, \n",
    "# scaling, etc. Remove variables that are not needed/useful for the analysis. \n",
    "# Describe the final dataset that is used for classification/regression\n",
    "display(X.info())\n",
    "display(y.info())\n",
    "# (include a description of any newly formed variables you created).\n",
    "# MAKE SURE TO NORMALIZE VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899bcb3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.3 Division of Trainig and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c72a38be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Divide your data into training and testing data using an 80% training \n",
    "# and 20% testing split. Use the cross validation modules that are part \n",
    "# of scikit-learn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22058f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Argue \"for\" or \"against\" splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "### 2. Modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "7254912c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1 One-Versus-All Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ff4c3856",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "\n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "\n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "\n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate\n",
    "            # add bacause maximizing\n",
    "\n",
    "# for this, we won't perform our own BFGS implementation\n",
    "# (it takes a fair amount of code and understanding, which we haven't setup yet)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs # maybe the most common bfgs algorithm in the world\n",
    "from numpy import ma\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "        return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(w**2)\n",
    "        #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "\n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "\n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "        return gradient\n",
    "\n",
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20,\n",
    "                 C=0.0001,\n",
    "                 solver=BFGSBinaryLogisticRegression):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = np.array(y==yval).astype(int) # create a binary problem\n",
    "\n",
    "            # train the binary classifier for this class\n",
    "\n",
    "            hblr = self.solver(eta=self.eta,iterations=self.iters,C=self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "\n",
    "        return np.hstack(probs) # make into single matrix\n",
    "\n",
    "    def predict(self,X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X),axis=1)] # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-0.04930964 -0.13381531 -0.00209226 -0.01006834 -0.0838108  -0.00059989\n",
      "   0.01787291 -0.00294707 -0.03349794 -0.10861926 -0.01787619 -0.37767188]\n",
      " [-0.00922379  0.06277083  0.03911189 -0.0115826  -0.04957062  0.00149078\n",
      "  -0.06508956  0.00394881 -0.0045812  -0.01618247 -0.0070719  -0.20846836]\n",
      " [ 0.06986028  0.34329671  0.09536574  0.00733588 -0.01273653  0.01468912\n",
      "  -0.01208132  0.0097361   0.06640493  0.16743208 -0.01753883 -0.46702099]\n",
      " [-0.01932771 -0.08663188 -0.13958461  0.03052666  0.01402905 -0.0031808\n",
      "   0.00509632 -0.00225901 -0.01763659 -0.05625123  0.00935413  0.06428658]\n",
      " [-0.09413913 -0.41986038 -0.05756825 -0.02153922 -0.00539694 -0.01730121\n",
      "   0.0119446  -0.01046589 -0.08462634 -0.22946473  0.00769617  0.3044183 ]\n",
      " [-0.03834349 -0.31781933 -0.01097037 -0.01139994  0.00327578 -0.00449756\n",
      "   0.02743831 -0.0189195  -0.03791695 -0.10079116 -0.01597708  0.06660213]\n",
      " [-0.06180066 -0.22031426 -0.01468611 -0.00531877 -0.24664119 -0.00271916\n",
      "   0.02496159 -0.00486269 -0.03976363 -0.10693683 -0.01576348 -0.28435756]]\n",
      "Accuracy of:  0.4704312114989733\n",
      "[[   5  995]\n",
      " [   6 3848]\n",
      " [   7   27]]\n",
      "[[   3   20]\n",
      " [   4  162]\n",
      " [   5 1448]\n",
      " [   6 2186]\n",
      " [   7  875]\n",
      " [   8  174]\n",
      " [   9    5]]\n",
      "CPU times: total: 1.22 s\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=1,\n",
    "                                  iterations=10,\n",
    "                                  C=0.01,\n",
    "                                  solver=BFGSBinaryLogisticRegression\n",
    "                                  )\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))\n",
    "unique_yhat, counts_yhat = np.unique(yhat, return_counts=True)\n",
    "unique_y, counts_y = np.unique(y, return_counts=True)\n",
    "print(np.asarray((unique_yhat, counts_yhat)).T)\n",
    "print(np.asarray((unique_y, counts_y)).T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.69492099e-01  5.40250299e-01  7.96203917e-01 -4.74192393e-01\n",
      "  -8.81037466e-02  5.07314375e-02  2.58806856e-02  1.51678776e-03\n",
      "  -6.63247741e-01 -1.29113450e+00 -5.84990819e-01 -4.25845876e-01]\n",
      " [-7.33792911e-02  2.92105683e-01  4.39657377e+00 -6.06222872e-01\n",
      "  -8.26519489e-02  1.03392357e-01 -4.79793403e-02 -1.39052006e-03\n",
      "  -4.61493085e-02  4.54103967e-01 -1.16343142e-01 -5.58478324e-01]\n",
      " [ 3.01029025e+00  1.14120058e-01  3.61362275e+00  2.31290486e-01\n",
      "  -4.41178025e-02  5.41961314e-01 -8.48962106e-03  4.28860584e-03\n",
      "   2.99587148e+00  2.64637655e-01 -1.20601146e+00 -8.78290529e-01]\n",
      " [-7.59539105e-03 -6.19469730e-02 -3.35565623e+00  1.72983062e-01\n",
      "   2.28163648e-02  2.79529409e-01  7.21265670e-04 -1.41656905e-04\n",
      "   1.18852463e-02 -1.95127236e-01  1.61095011e-01  1.38691202e-01]\n",
      " [-3.52750737e+00 -1.05773142e-01 -3.01386648e+00 -7.04012803e-01\n",
      "   2.81016566e-02 -1.44024018e+00  8.81147025e-03 -3.99916153e-03\n",
      "  -3.53583853e+00 -1.88214372e-01  1.12173914e+00  6.89559439e-01]\n",
      " [-2.04001859e+00 -3.57989176e-01 -1.29379238e+00 -1.36671771e-01\n",
      "   4.89589156e-02 -3.24749374e-01  1.90337891e-02 -5.01594982e-03\n",
      "  -2.03879055e+00 -1.27390349e+00 -1.59467539e-01  6.76854945e-01]\n",
      " [-4.93192823e-01 -9.18037723e-02  9.98496637e-03  1.40874505e-01\n",
      "  -8.30195248e-02 -9.41743191e-02  2.63850715e-02 -2.47428859e-02\n",
      "  -4.92940874e-01 -9.09531815e-01 -2.88554418e-01  3.15064780e-02]]\n",
      "Accuracy of:  0.5316221765913758\n",
      "CPU times: total: 297 ms\n",
      "Wall time: 188 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='liblinear') # all params default\n",
    "\n",
    "lr_sk.fit(X,y)\n",
    "print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "6102f27e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. Training Classifier for Good Generalization Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b8f60a34",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc542313",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349ddd42",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.3 Comparing Best Performing Procedure to Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d0e1ada2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the performance differences in terms of training time and classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038b3ee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Discuss the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c06cb74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a2bb8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76f603",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4. BFGS (Can change but thought this would be better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "75888c7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Implementation of BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33beecb4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compare your performance accuracy and runtime to the BFGS implementation in SciPy (that we used in lecture). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}