{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe86ed7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab Assignment Two: Exploring Image Data \n",
    "\n",
    "#### Everett Cienkus, Blake Miller, Colin Weil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adba831",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Business Understanding\n",
    "\n",
    "The data set is a collection of images of 13 different types of fruits and vegetables. These are 100x100 px images with a single food item and a white background. There are a total of 6938 images to train and 2317 images to test the accuracy of the data set. This image data was collected by Mihai Oltean for the purpose of documenting the 360 degree views of fruits. These fruits were filmed using a stationary camera, a slow rotating motor, and a white background. The different food items were placed on a rod attached to the motor and slowly spun while the camera captured the different angles of the food item. The white background was not consistent throughout the photos and negatively impacted the ability to declare the food item from the background. To combat this the collectors created an algorithm that removed the background of the images and replaced it with the color white.\n",
    "\n",
    "The prediction task for this dataset is to use the images of food items in the train set to identify the food items in the images in the test data. Third parties that would be interested in these results would be someone creating a mobile app that allows the user to identify food items. Typically, a person would know the different food items within their own home and grocery stores label their items, leaving no need for the app in these situations. However, when people travel to foreign countries, they may see food items on a tree or bush or vine and want to identify this item. Farmers could also program this intelligence into their machines, allowing automatic picking machines to identify where these food items are or how many are left that have not been harvested.\n",
    "\n",
    "Right now, Google's image recognition has an accuracy of 94%, so we would want to meet this level for our algorithm to be considered useful to any third party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 2. Data Preparation "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df72f3ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy import asarray\n",
    "import pandas as pd\n",
    "    \n",
    "changeListToSingleValue_vec = lambda i: int(''.join(map(str, i)))\n",
    "\n",
    "def get_data(dir):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "         for file in files:\n",
    "            with open(os.path.join(root, file), \"r\") as auto:\n",
    "                if file.lower().endswith('.jpg'):\n",
    "                    # Read in image as numpy arrays\n",
    "                    image = Image.open(os.path.join(root, file))\n",
    "                    # Recolor to black and white and flatten to 1D array\n",
    "                    np_img = np.array(image.convert('L')).flatten()\n",
    "                    # Add to list of images\n",
    "                    images.append(np_img)\n",
    "                    # Adds what fruit the image is to target list\n",
    "                    targets.append(root[len(dir)+1:len(root)])\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get training and test data\n",
    "images, targets = get_data('fruit_dataset/small_train')\n",
    "X_test, y_test = get_data('fruit_dataset/small_test')\n",
    "print(len(X_test), 'test images loaded.')\n",
    "print(len(images), ' training pictures loaded. Examples: ')\n",
    "# Visualize Several Images\n",
    "display(Image.fromarray(images[0].reshape(100,100)))\n",
    "display(Image.fromarray(images[500].reshape(100,100)))\n",
    "display(Image.fromarray(images[1000].reshape(100,100)))\n",
    "display(Image.fromarray(images[1500].reshape(100,100)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Data Reduction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1 Linear Dimensionality Reduction PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# manipulated from Sebastian Raschka Example (your book!)\n",
    "# also from hi blog here: http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html\n",
    "\n",
    "# this is a scree plot\n",
    "def plot_explained_variance(pca):\n",
    "    import plotly\n",
    "    from plotly.graph_objs import Bar, Line\n",
    "    from plotly.graph_objs import Scatter, Layout\n",
    "    from plotly.graph_objs.scatter import Marker\n",
    "    from plotly.graph_objs.layout import XAxis, YAxis\n",
    "    plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cum_var_exp = np.cumsum(explained_var)\n",
    "\n",
    "    plotly.offline.iplot({\n",
    "        \"data\": [Bar(y=explained_var, name='individual explained variance'),\n",
    "                 Scatter(y=cum_var_exp, name='cumulative explained variance')\n",
    "                 ],\n",
    "        \"layout\": Layout(xaxis=XAxis(title='Principal components'), yaxis=YAxis(title='Explained variance ratio'))\n",
    "    })\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = images\n",
    "y = targets\n",
    "n_components = 300\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "%time pca.fit(X.copy())\n",
    "plot_explained_variance(pca)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see from the figure above, at around 150 components we can explain about 96% of the variation, and the graph begins to become extremely flat. With 300 components the explained variance gets up to 97.7%, but we found that 150 components is all that is needed to get accurate results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 Linear Dimensionality Reduction Randomized PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rand_pca = PCA(n_components=n_components, svd_solver='randomized')\n",
    "%time rand_pca.fit(X.copy())\n",
    "plot_explained_variance(rand_pca)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again we can see from the figure above, at around 150 components we can explain about 96% of the variation. With 300 components the explained variance gets up to 97.7%, but we found that 150 components is all that is needed to get accurate results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Compare PCA and Randomized PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "\n",
    "# get training PCA features\n",
    "pca_features = pca.transform(copy.deepcopy(X))\n",
    "rand_pca_features = rand_pca.transform(copy.deepcopy(X))\n",
    "\n",
    "# transform test data\n",
    "pca_test = pca.transform(copy.deepcopy(X_test))\n",
    "rand_pca_test = rand_pca.transform(copy.deepcopy(X_test))\n",
    "\n",
    "#get KNN accuracy\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_random_pca = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn_pca.fit(pca_features,y)\n",
    "acc_pca = accuracy_score(knn_pca.predict(pca_test),y_test)\n",
    "\n",
    "knn_random_pca.fit(rand_pca_features,y)\n",
    "acc_rand_pca = accuracy_score(knn_random_pca.predict(rand_pca_test),y_test)\n",
    "\n",
    "print(f\"PCA accuracy:{100*acc_pca:.2f}%, Random PCA Accuracy:{100*acc_rand_pca:.2f}%\".format())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the graphs of explained variance, along with actual classification performance of the two methods, we can see that there is not a very large difference between using PCA vs randomized PCA for this dataset. At 300 components, they have an explained variance difference of just 0.0001%. The PCA did seem to classify very slightly more accurately, but took longer to calculate. Randomized PCA would be better to use because of the slight performance increase, especially if we were using larger images and datasets. Neither one of these methods meets the 94% accuracy that we would hope for when classifying these images."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.4 DAISY - Feature Extraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from skimage.feature import daisy\n",
    "from skimage.io import imshow\n",
    "\n",
    "img = Image.fromarray(images[0].reshape(100,100))\n",
    "\n",
    "# Visualize what the daisy descriptor looks like\n",
    "features, img_desc = daisy(img, step=20, radius=25, rings=2, histograms=8, orientations=8, visualize=True)\n",
    "imshow(img_desc)\n",
    "plt.grid(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_daisy(row,shape):\n",
    "    feat = daisy(row.reshape(shape), step=20, radius=25, rings=2, histograms=8, orientations=8, visualize=False)\n",
    "    return feat.reshape((-1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%time daisy_features = np.apply_along_axis(apply_daisy, 1, X, (100,100))\n",
    "print(daisy_features.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_dsy = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "#fit test data\n",
    "dsy_test = np.apply_along_axis(apply_daisy, 1, X_test, (100,100))\n",
    "\n",
    "knn_dsy.fit(daisy_features,y)\n",
    "acc_dsy = accuracy_score(knn_dsy.predict(dsy_test),y_test)\n",
    "\n",
    "print(f\"PCA accuracy:{100*acc_pca:.2f}%, Daisy Accuracy:{100*acc_dsy:.2f}%\".format())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From our nearest neighbor classifier, we can see that DAISY only slightly outperforms our PCA accuracy. A 92.66% accuracy isn't horrible, but it is likely worse than a human would perform for the given fruits, and it still doesn' meet the 94% that Google image recognition meets. Tweaking the step, radius, could allow for more accuracy, but increasing the step and decreasing the radius can make the DAISY calculation very slow."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. DAISY - Key Point Matching"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Code taken directly from exceptional credit section of Image Data notebook\n",
    "from skimage.feature import match_descriptors\n",
    "\n",
    "def apply_daisy(row,shape): # no reshape in this function\n",
    "    feat = daisy(row.reshape(shape), step=10, radius=10,\n",
    "                 rings=1, histograms=8, orientations=4,\n",
    "                 visualize=False)\n",
    "    s = feat.shape # PxQxR\n",
    "    #P = ceil((Height - radius*2) / step)\n",
    "    #Q = ceil((Width - radius*2) / step)\n",
    "    #R = (rings * histograms + 1) * orientations\n",
    "    return feat.reshape((s[0]*s[1],s[2]))\n",
    "\n",
    "d1 = apply_daisy(X[0],(100,100))\n",
    "d2 = apply_daisy(X[1],(100,100))\n",
    "d3 = apply_daisy(X[1000],(100,100))\n",
    "print(d1.shape, d2.shape, d3.shape)\n",
    "print('Classes:',y[0],y[1],y[1000])\n",
    "\n",
    "# return list of the key points indices that matched closely enough\n",
    "matches = match_descriptors(d1, d2, cross_check=True, max_ratio=0.8)\n",
    "print(f\"Number of matches, same class: {matches.shape[0]}, Percentage:{100*matches.shape[0]/len(d1):0.2f}%\")\n",
    "\n",
    "# return list of the key points indices that matched closely enough\n",
    "matches = match_descriptors(d1, d3, cross_check=True, max_ratio=0.8)\n",
    "print(f\"Number of matches, diff classes: {matches.shape[0]}, Percentage:{100*matches.shape[0]/len(d1):0.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get smaller tst and train dataset \n",
    "X, y = get_data('fruit_dataset/tiny_train')\n",
    "X_test, y_test = get_data('fruit_dataset/tiny_test')\n",
    "#get key points of test and train data\n",
    "key_point_train = np.apply_along_axis(apply_daisy, 1, X, (100,100))\n",
    "key_point_test = np.apply_along_axis(apply_daisy, 1, X_test, (100,100))\n",
    "print(key_point_train.shape)\n",
    "print(key_point_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function returns a list of the indexes of closest image in the training set to each given element of the test set\n",
    "def get_matches(train,test):\n",
    "    idxList = []\n",
    "    for i in range(test.shape[0]):\n",
    "        max = 0.0\n",
    "        idx = 0\n",
    "        for j in range(train.shape[0]):\n",
    "            matches = match_descriptors(test[i], train[j] , cross_check=True, max_ratio=0.8)\n",
    "            if matches.shape[0]>max:\n",
    "                # save index of max match\n",
    "                max = matches.shape[0]\n",
    "                idx = j\n",
    "        idxList.append(idx)\n",
    "    return idxList\n",
    "%time idxList = get_matches(key_point_train,key_point_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# find how many were matched correctly\n",
    "label_list = [y[i] for i in idxList]\n",
    "count = 0\n",
    "for k in range(len(label_list)):\n",
    "    if label_list[k] == y_test[k]:\n",
    "        count +=1\n",
    "print(f\"Accuracy:{100*count/len(y_test):0.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The key point matching does seem to work more accurately than the daisy alone (93.53% vs. 92.66%), but the dataset had to be greatly reduced in order to perform the brute force matching that was implemented in the code above. While an accuracy of 93.53% might seem good, and it is very close to the desired 94%, the training dataset was reduced to about 1900 images of just 4 types of fruit vs. the original 7000 with 13 types of fruit. The runtime was 3 min, and because of the nested for loops used in this method, increasing the size of the training data or test data greatly increases the execution time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}