{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe86ed7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab Assignment Five: Wide and Deep Network Architectures\n",
    " \n",
    "\n",
    "#### Everett Cienkus, Blake Miller, Colin Weil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adba831",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb0229",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.1 Define and Prepare Class Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1d91d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data from https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction\n",
    "\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9dff9dc2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  \\\n0     40   M           ATA        140          289          0     Normal   \n1     49   F           NAP        160          180          0     Normal   \n2     37   M           ATA        130          283          0         ST   \n3     48   F           ASY        138          214          0     Normal   \n4     54   M           NAP        150          195          0     Normal   \n..   ...  ..           ...        ...          ...        ...        ...   \n913   45   M            TA        110          264          0     Normal   \n914   68   M           ASY        144          193          1     Normal   \n915   57   M           ASY        130          131          0     Normal   \n916   57   F           ATA        130          236          0        LVH   \n917   38   M           NAP        138          175          0     Normal   \n\n     MaxHR ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n0      172              N      0.0       Up             0  \n1      156              N      1.0     Flat             1  \n2       98              N      0.0       Up             0  \n3      108              Y      1.5     Flat             1  \n4      122              N      0.0       Up             0  \n..     ...            ...      ...      ...           ...  \n913    132              N      1.2     Flat             1  \n914    141              N      3.4     Flat             1  \n915    115              Y      1.2     Flat             1  \n916    174              N      0.0     Flat             1  \n917    173              N      0.0       Up             0  \n\n[918 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>ChestPainType</th>\n      <th>RestingBP</th>\n      <th>Cholesterol</th>\n      <th>FastingBS</th>\n      <th>RestingECG</th>\n      <th>MaxHR</th>\n      <th>ExerciseAngina</th>\n      <th>Oldpeak</th>\n      <th>ST_Slope</th>\n      <th>HeartDisease</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>M</td>\n      <td>ATA</td>\n      <td>140</td>\n      <td>289</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>172</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>F</td>\n      <td>NAP</td>\n      <td>160</td>\n      <td>180</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>156</td>\n      <td>N</td>\n      <td>1.0</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37</td>\n      <td>M</td>\n      <td>ATA</td>\n      <td>130</td>\n      <td>283</td>\n      <td>0</td>\n      <td>ST</td>\n      <td>98</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>48</td>\n      <td>F</td>\n      <td>ASY</td>\n      <td>138</td>\n      <td>214</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>108</td>\n      <td>Y</td>\n      <td>1.5</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54</td>\n      <td>M</td>\n      <td>NAP</td>\n      <td>150</td>\n      <td>195</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>122</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>913</th>\n      <td>45</td>\n      <td>M</td>\n      <td>TA</td>\n      <td>110</td>\n      <td>264</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>132</td>\n      <td>N</td>\n      <td>1.2</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>914</th>\n      <td>68</td>\n      <td>M</td>\n      <td>ASY</td>\n      <td>144</td>\n      <td>193</td>\n      <td>1</td>\n      <td>Normal</td>\n      <td>141</td>\n      <td>N</td>\n      <td>3.4</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>915</th>\n      <td>57</td>\n      <td>M</td>\n      <td>ASY</td>\n      <td>130</td>\n      <td>131</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>115</td>\n      <td>Y</td>\n      <td>1.2</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>916</th>\n      <td>57</td>\n      <td>F</td>\n      <td>ATA</td>\n      <td>130</td>\n      <td>236</td>\n      <td>0</td>\n      <td>LVH</td>\n      <td>174</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>917</th>\n      <td>38</td>\n      <td>M</td>\n      <td>NAP</td>\n      <td>138</td>\n      <td>175</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>173</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>918 rows Ã— 12 columns</p>\n</div>"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data into memory and save it to a pandas data frame.\n",
    "df_train = pd.read_csv('heart_dataset/heart.csv')\n",
    "\n",
    "# Remove any observations that having missing data.\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the following 11 features:\n",
      "[   'Sex_int',\n",
      "    'ChestPainType_int',\n",
      "    'FastingBS_int',\n",
      "    'RestingECG_int',\n",
      "    'ExerciseAngina_int',\n",
      "    'ST_Slope_int',\n",
      "    'Age',\n",
      "    'RestingBP',\n",
      "    'Cholesterol',\n",
      "    'MaxHR',\n",
      "    'Oldpeak']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# ========================================================\n",
    "# define objects that can encode each variable as integer\n",
    "encoders = dict() # save each encoder in dictionary\n",
    "categorical_headers = ['Sex','ChestPainType','FastingBS','RestingECG','ExerciseAngina','ST_Slope']\n",
    "\n",
    "# train all encoders\n",
    "for col in categorical_headers:\n",
    "    encoders[col] = LabelEncoder() # save the encoder\n",
    "    df_train[col+'_int'] = encoders[col].fit_transform(df_train[col])\n",
    "\n",
    "# ========================================================\n",
    "# scale the numeric, continuous variables\n",
    "numeric_headers = ['Age','RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "ss = StandardScaler()\n",
    "df_train[numeric_headers] = ss.fit_transform(df_train[numeric_headers].values)\n",
    "\n",
    "\n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "\n",
    "feature_columns = categorical_headers_ints+numeric_headers\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "print(f\"We will use the following {len(feature_columns)} features:\")\n",
    "pp.pprint(feature_columns)\n",
    "\n",
    "X_train = df_train[feature_columns].to_numpy()\n",
    "y_train = df_train['HeartDisease']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "79e4c590",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2 Combine into Cross-Product Features"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Identify groups of features in your data that should be combined into cross-product features. Provide justification for why these features should be crossed (or why some features should not be crossed):\n",
    "\n",
    "One of the crosses we decided to use is crossing the ExerciseAngina column and the ChestPainType column. We decided to cross these because they are both attributes that relate to chest pain, as the ExerciseAngina column describes a specific kind of chest pain."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex has 2 unique values:\n",
      "['M' 'F']\n",
      "ChestPainType has 4 unique values:\n",
      "['ATA' 'NAP' 'ASY' 'TA']\n",
      "FastingBS has 2 unique values:\n",
      "[0 1]\n",
      "RestingECG has 3 unique values:\n",
      "['Normal' 'ST' 'LVH']\n",
      "ExerciseAngina has 2 unique values:\n",
      "['N' 'Y']\n",
      "ST_Slope has 3 unique values:\n",
      "['Up' 'Flat' 'Down']\n"
     ]
    }
   ],
   "source": [
    "for col in categorical_headers:\n",
    "    vals = df_train[col].unique()\n",
    "    print(col,'has', len(vals), 'unique values:')\n",
    "    print(vals)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "data": {
      "text/plain": "['ChestPainType_ExerciseAngina', 'RestingECG_ST_Slope']"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a quick example of crossing some columns\n",
    "\n",
    "cross_columns = [#['race','sex','education','occupation'],\n",
    "    ['ChestPainType','ExerciseAngina'],\n",
    "    ['RestingECG','ST_Slope'],\n",
    "]\n",
    "\n",
    "# cross each set of columns in the list above\n",
    "cross_col_df_names = []\n",
    "for cols_list in cross_columns:\n",
    "    # encode as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "\n",
    "    # 1. create crossed labels by join operation\n",
    "    X_crossed_train = df_train[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "    # get a nice name for this new crossed column\n",
    "    cross_col_name = '_'.join(cols_list)\n",
    "\n",
    "    # 2. encode as integers, stacking all possibilities\n",
    "    enc.fit(X_crossed_train.to_numpy())\n",
    "\n",
    "    # 3. Save into dataframe with new name\n",
    "    df_train[cross_col_name] = enc.transform(X_crossed_train)\n",
    "\n",
    "    # keep track of the new names of the crossed columns\n",
    "    cross_col_df_names.append(cross_col_name)\n",
    "\n",
    "cross_col_df_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "dac6fafd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.3 Choose Metrics to Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccfad55",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fef24f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.4 Choose Method for Dividing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8afae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Argue why your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ee25b953",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# df_train, df_test = train_test_split(df, test_size=0.2, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8ed74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since our dataset is over 50,000, it is okay to use 80/20 split. EXPAND ON THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701fbe1a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6d2f1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1 Create Three Combined Wide and Deep Netowkrs using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c9e0a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create at least three combined wide and deep networks to classify your data using Keras. Visualize the performance of the network on the training data and validation data in the same plot versus the training iterations. Note: use the \"history\" return parameter that is part of Keras \"fit\" function to easily access this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import concatenate\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "# get crossed columns\n",
    "X_train_crossed = df_train[cross_col_df_names].to_numpy()\n",
    "\n",
    "# save categorical features\n",
    "X_train_cat = df_train[categorical_headers_ints].to_numpy()\n",
    "\n",
    "# and save off the numeric features\n",
    "X_train_num =  df_train[numeric_headers].to_numpy()\n",
    "\n",
    "\n",
    "# we need to create separate lists for each branch\n",
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "\n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = df_train[col].max()+1\n",
    "\n",
    "\n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "\n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "\n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "\n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = df_train[col].max()+1\n",
    "\n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "\n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "\n",
    "# NUMERIC DATA INPUT\n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "\n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "\n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_crossed,input_cat,input_num],\n",
    "              outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "990d48fe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.2 Investigate Performance by Altering the Number of Layers in the Deep Branch of the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2a928",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Investigate generalization performance by altering the number of layers in the deep branch of the network. Try at least two different number of layers. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab to select the number of layers that performs superiorly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9e991851",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "29/29 [==============================] - 1s 1ms/step - loss: 0.2476 - accuracy: 0.5403\n",
      "Epoch 2/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2442 - accuracy: 0.5577\n",
      "Epoch 3/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2408 - accuracy: 0.5643\n",
      "Epoch 4/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2374 - accuracy: 0.5730\n",
      "Epoch 5/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.5763\n",
      "Epoch 6/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2309 - accuracy: 0.5850\n",
      "Epoch 7/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2278 - accuracy: 0.5959\n",
      "Epoch 8/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2246 - accuracy: 0.6089\n",
      "Epoch 9/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2215 - accuracy: 0.6285\n",
      "Epoch 10/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.6460\n",
      "Epoch 11/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.6765\n",
      "Epoch 12/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2122 - accuracy: 0.6863\n",
      "Epoch 13/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.7059\n",
      "Epoch 14/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2061 - accuracy: 0.7233\n",
      "Epoch 15/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2032 - accuracy: 0.7429\n",
      "Epoch 16/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2003 - accuracy: 0.7560\n",
      "Epoch 17/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1975 - accuracy: 0.7603\n",
      "Epoch 18/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1948 - accuracy: 0.7723\n",
      "Epoch 19/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1921 - accuracy: 0.7734\n",
      "Epoch 20/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1896 - accuracy: 0.7767\n",
      "Epoch 21/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1871 - accuracy: 0.7854\n",
      "Epoch 22/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1848 - accuracy: 0.7865\n",
      "Epoch 23/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1825 - accuracy: 0.7941\n",
      "Epoch 24/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1803 - accuracy: 0.7952\n",
      "Epoch 25/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1783 - accuracy: 0.7941\n",
      "Epoch 26/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1763 - accuracy: 0.7952\n",
      "Epoch 27/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1744 - accuracy: 0.7952\n",
      "Epoch 28/30\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1725 - accuracy: 0.7996\n",
      "Epoch 29/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1707 - accuracy: 0.8050\n",
      "Epoch 30/30\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1690 - accuracy: 0.8072\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2118c8dad08>"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d3a88",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.3 Investigate Performance of the Best Wide and Deep Network to Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed9555",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP). Alternatively, you can compare to a network without the wide branch (i.e., just the deep network). For classification tasks, compare using the receiver operating characteristic and area under the curve. For regression tasks, use Bland-Altman plots and residual variance calculations.  Use proper statistical methods to compare the performance of different models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8c9fc853",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be73ef7e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. Capturing the Embedding Weights from the Deep Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7912e89",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Capture the embedding weights from the deep network and (if needed) perform dimensionality reduction on the output of these embedding layers (only if needed). That is, pass the observations into the network, save the embedded weights (called embeddings), and then perform  dimensionality reduction in order to visualize results. Visualize and explain any clusters in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "957af2da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}