{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe86ed7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab Assignment Five: Wide and Deep Network Architectures\n",
    " \n",
    "\n",
    "#### Everett Cienkus, Blake Miller, Colin Weil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adba831",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb0229",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.1 Define and Prepare Class Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1d91d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data from https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction\n",
    "\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "       employee_id         department     region         education gender  \\\n0            65438  Sales & Marketing   region_7  Master's & above      f   \n1            65141         Operations  region_22        Bachelor's      m   \n2             7513  Sales & Marketing  region_19        Bachelor's      m   \n3             2542  Sales & Marketing  region_23        Bachelor's      m   \n4            48945         Technology  region_26        Bachelor's      m   \n...            ...                ...        ...               ...    ...   \n54802         6915  Sales & Marketing  region_14        Bachelor's      m   \n54803         3030         Technology  region_14        Bachelor's      m   \n54804        74592         Operations  region_27  Master's & above      f   \n54805        13918          Analytics   region_1        Bachelor's      m   \n54807        51526                 HR  region_22        Bachelor's      m   \n\n      recruitment_channel  no_of_trainings  age  previous_year_rating  \\\n0                sourcing                1   35                   5.0   \n1                   other                1   30                   5.0   \n2                sourcing                1   34                   3.0   \n3                   other                2   39                   1.0   \n4                   other                1   45                   3.0   \n...                   ...              ...  ...                   ...   \n54802               other                2   31                   1.0   \n54803            sourcing                1   48                   3.0   \n54804               other                1   37                   2.0   \n54805               other                1   27                   5.0   \n54807               other                1   27                   1.0   \n\n       length_of_service  awards_won?  avg_training_score  is_promoted  \n0                      8            0                  49            0  \n1                      4            0                  60            0  \n2                      7            0                  50            0  \n3                     10            0                  50            0  \n4                      2            0                  73            0  \n...                  ...          ...                 ...          ...  \n54802                  2            0                  49            0  \n54803                 17            0                  78            0  \n54804                  6            0                  56            0  \n54805                  3            0                  79            0  \n54807                  5            0                  49            0  \n\n[48660 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>employee_id</th>\n      <th>department</th>\n      <th>region</th>\n      <th>education</th>\n      <th>gender</th>\n      <th>recruitment_channel</th>\n      <th>no_of_trainings</th>\n      <th>age</th>\n      <th>previous_year_rating</th>\n      <th>length_of_service</th>\n      <th>awards_won?</th>\n      <th>avg_training_score</th>\n      <th>is_promoted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>65438</td>\n      <td>Sales &amp; Marketing</td>\n      <td>region_7</td>\n      <td>Master's &amp; above</td>\n      <td>f</td>\n      <td>sourcing</td>\n      <td>1</td>\n      <td>35</td>\n      <td>5.0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>49</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>65141</td>\n      <td>Operations</td>\n      <td>region_22</td>\n      <td>Bachelor's</td>\n      <td>m</td>\n      <td>other</td>\n      <td>1</td>\n      <td>30</td>\n      <td>5.0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>60</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7513</td>\n      <td>Sales &amp; Marketing</td>\n      <td>region_19</td>\n      <td>Bachelor's</td>\n      <td>m</td>\n      <td>sourcing</td>\n      <td>1</td>\n      <td>34</td>\n      <td>3.0</td>\n      <td>7</td>\n      <td>0</td>\n      <td>50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2542</td>\n      <td>Sales &amp; Marketing</td>\n      <td>region_23</td>\n      <td>Bachelor's</td>\n      <td>m</td>\n      <td>other</td>\n      <td>2</td>\n      <td>39</td>\n      <td>1.0</td>\n      <td>10</td>\n      <td>0</td>\n      <td>50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>48945</td>\n      <td>Technology</td>\n      <td>region_26</td>\n      <td>Bachelor's</td>\n      <td>m</td>\n      <td>other</td>\n      <td>1</td>\n      <td>45</td>\n      <td>3.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>73</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>54802</th>\n      <td>6915</td>\n      <td>Sales &amp; Marketing</td>\n      <td>region_14</td>\n      <td>Bachelor's</td>\n      <td>m</td>\n      <td>other</td>\n      <td>2</td>\n      <td>31</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>49</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54803</th>\n      <td>3030</td>\n      <td>Technology</td>\n      <td>region_14</td>\n      <td>Bachelor's</td>\n      <td>m</td>\n      <td>sourcing</td>\n      <td>1</td>\n      <td>48</td>\n      <td>3.0</td>\n      <td>17</td>\n      <td>0</td>\n      <td>78</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54804</th>\n      <td>74592</td>\n      <td>Operations</td>\n      <td>region_27</td>\n      <td>Master's &amp; above</td>\n      <td>f</td>\n      <td>other</td>\n      <td>1</td>\n      <td>37</td>\n      <td>2.0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>56</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54805</th>\n      <td>13918</td>\n      <td>Analytics</td>\n      <td>region_1</td>\n      <td>Bachelor's</td>\n      <td>m</td>\n      <td>other</td>\n      <td>1</td>\n      <td>27</td>\n      <td>5.0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>79</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54807</th>\n      <td>51526</td>\n      <td>HR</td>\n      <td>region_22</td>\n      <td>Bachelor's</td>\n      <td>m</td>\n      <td>other</td>\n      <td>1</td>\n      <td>27</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>49</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>48660 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data into memory and save it to a pandas data frame.\n",
    "df_train = pd.read_csv('promotion_dataset/train.csv')\n",
    "# Remove any observations that having missing data.\n",
    "df_train = df_train.dropna()\n",
    "df_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the following 10 features:\n",
      "[   'department_int',\n",
      "    'region_int',\n",
      "    'education_int',\n",
      "    'gender_int',\n",
      "    'recruitment_channel_int',\n",
      "    'no_of_trainings',\n",
      "    'previous_year_rating',\n",
      "    'length_of_service',\n",
      "    'awards_won?',\n",
      "    'avg_training_score']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# ========================================================\n",
    "# define objects that can encode each variable as integer\n",
    "encoders = dict() # save each encoder in dictionary\n",
    "categorical_headers = ['department','region','education','gender','recruitment_channel']\n",
    "# train all encoders\n",
    "for col in categorical_headers:\n",
    "    df_train[col] = df_train[col].str.strip()\n",
    "    encoders[col] = LabelEncoder() # save the encoder\n",
    "    df_train[col+'_int'] = encoders[col].fit_transform(df_train[col])\n",
    "\n",
    "# ========================================================\n",
    "# scale the numeric, continuous variables\n",
    "numeric_headers = ['no_of_trainings', 'previous_year_rating', 'length_of_service', 'awards_won?', 'avg_training_score']\n",
    "ss = StandardScaler()\n",
    "df_train[numeric_headers] = ss.fit_transform(df_train[numeric_headers].values)\n",
    "\n",
    "\n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers]\n",
    "\n",
    "feature_columns = categorical_headers_ints+numeric_headers\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "print(f\"We will use the following {len(feature_columns)} features:\")\n",
    "pp.pprint(feature_columns)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "79e4c590",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2 Combine into Cross-Product Features"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Identify groups of features in your data that should be combined into cross-product features. Provide justification for why these features should be crossed (or why some features should not be crossed):\n",
    "\n",
    "One of the crosses we decided to use is crossing the ExerciseAngina column and the ChestPainType column. We decided to cross these because they are both attributes that relate to chest pain, as the ExerciseAngina column describes a specific kind of chest pain."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department has 9 unique values:\n",
      "['Sales & Marketing' 'Operations' 'Technology' 'Analytics' 'R&D'\n",
      " 'Procurement' 'Finance' 'HR' 'Legal']\n",
      "region has 34 unique values:\n",
      "['region_7' 'region_22' 'region_19' 'region_23' 'region_26' 'region_2'\n",
      " 'region_20' 'region_34' 'region_1' 'region_4' 'region_29' 'region_31'\n",
      " 'region_15' 'region_14' 'region_11' 'region_5' 'region_28' 'region_17'\n",
      " 'region_13' 'region_16' 'region_25' 'region_10' 'region_27' 'region_30'\n",
      " 'region_12' 'region_21' 'region_32' 'region_6' 'region_33' 'region_8'\n",
      " 'region_24' 'region_3' 'region_9' 'region_18']\n",
      "education has 3 unique values:\n",
      "[\"Master's & above\" \"Bachelor's\" 'Below Secondary']\n",
      "gender has 2 unique values:\n",
      "['f' 'm']\n",
      "recruitment_channel has 3 unique values:\n",
      "['sourcing' 'other' 'referred']\n"
     ]
    }
   ],
   "source": [
    "for col in categorical_headers:\n",
    "    vals = df_train[col].unique()\n",
    "    print(col,'has', len(vals), 'unique values:')\n",
    "    print(vals)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['department_education', 'recruitment_channel_education']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a quick example of crossing some columns\n",
    "\n",
    "cross_columns = [#['race','sex','education','occupation'],\n",
    "    ['department','education'],\n",
    "    ['recruitment_channel','education']\n",
    "]\n",
    "\n",
    "# cross each set of columns in the list above\n",
    "cross_col_df_names = []\n",
    "for cols_list in cross_columns:\n",
    "    # encode as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "\n",
    "    # 1. create crossed labels by join operation\n",
    "    X_crossed_train = df_train[cols_list].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "    # get a nice name for this new crossed column\n",
    "    cross_col_name = '_'.join(cols_list)\n",
    "\n",
    "    # 2. encode as integers, stacking all possibilities\n",
    "    enc.fit(X_crossed_train.to_numpy())\n",
    "\n",
    "    # 3. Save into dataframe with new name\n",
    "    df_train[cross_col_name] = enc.transform(X_crossed_train)\n",
    "\n",
    "    # keep track of the new names of the crossed columns\n",
    "    cross_col_df_names.append(cross_col_name)\n",
    "\n",
    "cross_col_df_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "dac6fafd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.3 Choose Metrics to Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccfad55",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Choose and explain what metric(s) you will use to evaluate your algorithm’s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fef24f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.4 Choose Method for Dividing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8afae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Argue why your cross validation method is a realistic mirroring of how an algorithm would be used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train = df_train[feature_columns].to_numpy()\n",
    "y_train = df_train['is_promoted'].to_numpy()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "f6f8ed74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since our dataset is over 50,000, it is okay to use 80/20 split. EXPAND ON THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701fbe1a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6d2f1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1 Create Three Combined Wide and Deep Netowkrs using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c9e0a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create at least three combined wide and deep networks to classify your data using Keras. Visualize the performance of the network on the training data and validation data in the same plot versus the training iterations. Note: use the \"history\" return parameter that is part of Keras \"fit\" function to easily access this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import concatenate\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# get crossed columns\n",
    "X_train_crossed = df_train[cross_col_df_names].to_numpy()\n",
    "\n",
    "# save categorical features\n",
    "X_train_cat = df_train[categorical_headers_ints].to_numpy()\n",
    "\n",
    "# and save off the numeric features\n",
    "X_train_num =  df_train[numeric_headers].to_numpy()\n",
    "\n",
    "\n",
    "# we need to create separate lists for each branch\n",
    "crossed_outputs = []\n",
    "\n",
    "# CROSSED DATA INPUT\n",
    "input_crossed = Input(shape=(X_train_crossed.shape[1],), dtype='int64', name='wide_inputs')\n",
    "for idx,col in enumerate(cross_col_df_names):\n",
    "\n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = df_train[col].max()+1\n",
    "\n",
    "\n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_crossed, idx, axis=1)\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "\n",
    "    # save these outputs to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "\n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = concatenate(crossed_outputs, name='wide_concat')\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# CATEGORICAL DATA INPUT\n",
    "input_cat = Input(shape=(X_train_cat.shape[1],), dtype='int64', name='categorical_input')\n",
    "for idx,col in enumerate(categorical_headers_ints):\n",
    "\n",
    "    # track what the maximum integer value will be for this variable\n",
    "    # which is the same as the number of categories\n",
    "    N = df_train[col].max()+1\n",
    "\n",
    "    # this line of code does this: input_branch[:,idx]\n",
    "    x = tf.gather(input_cat, idx, axis=1)\n",
    "\n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N,\n",
    "                  output_dim=int(np.sqrt(N)),\n",
    "                  input_length=1, name=col+'_embed')(x)\n",
    "\n",
    "    # save these outputs to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "\n",
    "# NUMERIC DATA INPUT\n",
    "# create dense input branch for numeric\n",
    "input_num = Input(shape=(X_train_num.shape[1],), name='numeric')\n",
    "x_dense = Dense(units=22, activation='relu',name='num_1')(input_num)\n",
    "\n",
    "all_deep_branch_outputs.append(x_dense)\n",
    "\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "\n",
    "# merge the deep and wide branch\n",
    "final_branch = concatenate([wide_branch, deep_branch],\n",
    "                           name='concat_deep_wide')\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "model = Model(inputs=[input_crossed,input_cat,input_num],\n",
    "              outputs=final_branch)\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "990d48fe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.2 Investigate Performance by Altering the Number of Layers in the Deep Branch of the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2a928",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Investigate generalization performance by altering the number of layers in the deep branch of the network. Try at least two different number of layers. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab to select the number of layers that performs superiorly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e991851",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1521/1521 [==============================] - 10s 5ms/step - loss: 0.0959 - accuracy: 0.9125\n",
      "Epoch 2/30\n",
      "1521/1521 [==============================] - 8s 5ms/step - loss: 0.0791 - accuracy: 0.9130\n",
      "Epoch 3/30\n",
      "1521/1521 [==============================] - 5s 3ms/step - loss: 0.0765 - accuracy: 0.9130\n",
      "Epoch 4/30\n",
      "1521/1521 [==============================] - 9s 6ms/step - loss: 0.0740 - accuracy: 0.9130\n",
      "Epoch 5/30\n",
      "1521/1521 [==============================] - 5s 3ms/step - loss: 0.0726 - accuracy: 0.9159\n",
      "Epoch 6/30\n",
      "1521/1521 [==============================] - 5s 4ms/step - loss: 0.0720 - accuracy: 0.9168\n",
      "Epoch 7/30\n",
      "1521/1521 [==============================] - 9s 6ms/step - loss: 0.0716 - accuracy: 0.9167\n",
      "Epoch 8/30\n",
      "1521/1521 [==============================] - 6s 4ms/step - loss: 0.0713 - accuracy: 0.9169\n",
      "Epoch 9/30\n",
      "1521/1521 [==============================] - 6s 4ms/step - loss: 0.0711 - accuracy: 0.9169\n",
      "Epoch 10/30\n",
      "1521/1521 [==============================] - 8s 5ms/step - loss: 0.0709 - accuracy: 0.9170\n",
      "Epoch 11/30\n",
      "1521/1521 [==============================] - 5s 3ms/step - loss: 0.0706 - accuracy: 0.9172\n",
      "Epoch 12/30\n",
      "1521/1521 [==============================] - 7s 4ms/step - loss: 0.0704 - accuracy: 0.9173\n",
      "Epoch 13/30\n",
      "1521/1521 [==============================] - 8s 5ms/step - loss: 0.0702 - accuracy: 0.9174\n",
      "Epoch 14/30\n",
      "1521/1521 [==============================] - 5s 3ms/step - loss: 0.0700 - accuracy: 0.9176\n",
      "Epoch 15/30\n",
      "1521/1521 [==============================] - 8s 5ms/step - loss: 0.0698 - accuracy: 0.9177\n",
      "Epoch 16/30\n",
      "1521/1521 [==============================] - 7s 4ms/step - loss: 0.0695 - accuracy: 0.9183\n",
      "Epoch 17/30\n",
      "1521/1521 [==============================] - 5s 3ms/step - loss: 0.0692 - accuracy: 0.9189\n",
      "Epoch 18/30\n",
      "1521/1521 [==============================] - 9s 6ms/step - loss: 0.0688 - accuracy: 0.9193\n",
      "Epoch 19/30\n",
      "1521/1521 [==============================] - 6s 4ms/step - loss: 0.0684 - accuracy: 0.9199\n",
      "Epoch 20/30\n",
      "1521/1521 [==============================] - 6s 4ms/step - loss: 0.0679 - accuracy: 0.9208\n",
      "Epoch 21/30\n",
      "1521/1521 [==============================] - 9s 6ms/step - loss: 0.0672 - accuracy: 0.9217\n",
      "Epoch 22/30\n",
      "1521/1521 [==============================] - 5s 3ms/step - loss: 0.0665 - accuracy: 0.9229\n",
      "Epoch 23/30\n",
      "1521/1521 [==============================] - 7s 5ms/step - loss: 0.0656 - accuracy: 0.9239\n",
      "Epoch 24/30\n",
      "1521/1521 [==============================] - 7s 5ms/step - loss: 0.0648 - accuracy: 0.9244\n",
      "Epoch 25/30\n",
      "1521/1521 [==============================] - 5s 3ms/step - loss: 0.0639 - accuracy: 0.9257\n",
      "Epoch 26/30\n",
      "1521/1521 [==============================] - 9s 6ms/step - loss: 0.0629 - accuracy: 0.9269\n",
      "Epoch 27/30\n",
      "1521/1521 [==============================] - 5s 3ms/step - loss: 0.0619 - accuracy: 0.9285\n",
      "Epoch 28/30\n",
      "1521/1521 [==============================] - 6s 4ms/step - loss: 0.0608 - accuracy: 0.9299\n",
      "Epoch 29/30\n",
      "1521/1521 [==============================] - 9s 6ms/step - loss: 0.0596 - accuracy: 0.9314\n",
      "Epoch 30/30\n",
      "1521/1521 [==============================] - 5s 3ms/step - loss: 0.0584 - accuracy: 0.9330\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x219523eb408>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train_crossed,X_train_cat,X_train_num],\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d3a88",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.3 Investigate Performance of the Best Wide and Deep Network to Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed9555",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP). Alternatively, you can compare to a network without the wide branch (i.e., just the deep network). For classification tasks, compare using the receiver operating characteristic and area under the curve. For regression tasks, use Bland-Altman plots and residual variance calculations.  Use proper statistical methods to compare the performance of different models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c9fc853",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be73ef7e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. Capturing the Embedding Weights from the Deep Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7912e89",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Capture the embedding weights from the deep network and (if needed) perform dimensionality reduction on the output of these embedding layers (only if needed). That is, pass the observations into the network, save the embedded weights (called embeddings), and then perform  dimensionality reduction in order to visualize results. Visualize and explain any clusters in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "957af2da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}